{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHXNj-m4Ghwm"
      },
      "source": [
        "## **Classification Metrics (Binary)**\n",
        "\n",
        "### 1. **Confusion Matrix**\n",
        "\n",
        "A **Confusion Matrix** is a summary of prediction results that compares the actual (true) labels to the predicted labels made by a classifier. It provides insights into the number of true positives, false positives, true negatives, and false negatives.\n",
        "\n",
        "|                   | **Predicted Positive** | **Predicted Negative** |\n",
        "|-------------------|------------------------|------------------------|\n",
        "| **True Positive**  | **TP**                 | **FN**                 |\n",
        "| **True Negative**  | **FP**                 | **TN**                 |\n",
        "\n",
        "Where:\n",
        "- **TP** = True Positives (truely predicted positive cases),\n",
        "- **FP** = False Positives (falsely predicted positive cases),\n",
        "- **FN** = False Negatives (falsely predicted negative cases),\n",
        "- **TN** = True Negatives (truely predicted negative cases).\n",
        "\n",
        "This matrix helps in calculating other metrics like accuracy, precision, recall, F1-score, and specificity.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **Accuracy**\n",
        "\n",
        "**Accuracy** measures the percentage of correct predictions out of all predictions.\n",
        "\n",
        "$$\n",
        "\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}\n",
        "$$\n",
        "\n",
        "Where:\n",
        "- **TP** = True Positives,\n",
        "- **TN** = True Negatives,\n",
        "- **FP** = False Positives,\n",
        "- **FN** = False Negatives.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **Precision**\n",
        "\n",
        "**Precision** measures the proportion of true positive predictions out of all positive predictions (i.e., how many selected items are relevant).\n",
        "\n",
        "$$\n",
        "\\text{Precision} = \\frac{TP}{TP + FP}\n",
        "$$\n",
        "\n",
        "It is useful when the cost of false positives is high (e.g., in spam detection).\n",
        "\n",
        "---\n",
        "\n",
        "### 4. **Recall (Sensitivity or True Positive Rate)**\n",
        "\n",
        "**Recall** measures the proportion of actual positive cases that were correctly identified (i.e., how many relevant items are selected).\n",
        "\n",
        "$$\n",
        "\\text{Recall} = \\frac{TP}{TP + FN}\n",
        "$$\n",
        "\n",
        "It is important when the cost of false negatives is high (e.g., in medical diagnosis).\n",
        "\n",
        "---\n",
        "\n",
        "### 5. **Specificity (True Negative Rate)**\n",
        "\n",
        "**Specificity** measures the proportion of actual negative cases that were correctly identified. It is the complement of recall and focuses on the correct identification of negative cases.\n",
        "\n",
        "$$\n",
        "\\text{Specificity} = \\frac{TN}{TN + FP}\n",
        "$$\n",
        "\n",
        "Specificity is important in scenarios where false positives are costly, such as in medical tests.\n",
        "\n",
        "---\n",
        "\n",
        "### 6. **Balanced Accuracy**\n",
        "\n",
        "**Balanced Accuracy** adjusts for class imbalance by averaging the accuracy for both the positive and negative classes.\n",
        "\n",
        "$$\n",
        "\\text{Balanced Accuracy} = \\frac{1}{2} \\left( \\frac{TP}{TP + FN} + \\frac{TN}{TN + FP} \\right)\n",
        "$$\n",
        "\n",
        "This is particularly useful when working with imbalanced datasets, as it gives equal weight to the positive and negative classes.\n",
        "\n",
        "---\n",
        "\n",
        "### 7. **F1-Score**\n",
        "\n",
        "**F1-Score** is the harmonic mean of precision and recall, providing a single metric that balances both.\n",
        "\n",
        "$$\n",
        "F1 = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
        "$$\n",
        "\n",
        "F1-Score is useful when you need a balance between precision and recall."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "tlAHSrctErh6",
        "outputId": "0bd52fa6-d37f-4814-f8d9-6d82270b0d7c"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, balanced_accuracy_score\n",
        "\n",
        "# Create a synthetic binary classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, n_clusters_per_class=1, random_state=42)\n",
        "\n",
        "# Split the dataset into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Use a simple model (e.g., Logistic Regression) for classification\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "model = LogisticRegression(random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate confusion matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Calculate classification metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "specificity = conf_matrix[1, 1] / (conf_matrix[1, 1] + conf_matrix[0, 1]) # TN / (TN + FP)\n",
        "balanced_accuracy = balanced_accuracy_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "# Compile all metrics into a DataFrame\n",
        "metrics_df = pd.DataFrame({\n",
        "    'Metric': ['Accuracy', 'Precision', 'Recall', 'Specificity', 'Balanced Accuracy', 'F1 Score'],\n",
        "    'Score': [accuracy, precision, recall, specificity, balanced_accuracy, f1]\n",
        "})\n",
        "# Convert metrics to percentages and round to 1 decimal place\n",
        "metrics_df['Score'] = (metrics_df['Score'] * 100).round(1).astype(str) + '%'\n",
        "\n",
        "metrics_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 571
        },
        "id": "at7xrx3QHaCZ",
        "outputId": "ed9d49ae-4038-44ec-9c88-bfe8279fd3e1"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "# Normalize the confusion matrix to show percentages\n",
        "conf_matrix_normalized = conf_matrix / conf_matrix.sum(axis=1)[:, np.newaxis] * 100\n",
        "\n",
        "# Re-plotting the heatmap with both raw counts and percentages in the annotations\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "# Create a string formatter to add '%' symbol and raw counts to each value\n",
        "annotations = np.array([f'{value:.1f}%\\n({int(count)})' for value, count in zip(conf_matrix_normalized.flatten(), conf_matrix.flatten())]).reshape(2,2)\n",
        "\n",
        "sns.heatmap(conf_matrix_normalized, annot=annotations, fmt='', cmap='Blues', cbar_kws={'format': '%.0f%%'},\n",
        "            linewidths=1, linecolor='black', annot_kws={\"size\": 14})\n",
        "\n",
        "# Set axis labels and title\n",
        "plt.xlabel('Predicted Label', fontsize=14)\n",
        "plt.ylabel('True Label', fontsize=14)\n",
        "plt.title('Confusion Matrix with Normalized Values (%) and Counts', fontsize=16)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8ujN66my8ct"
      },
      "source": [
        "#### 7. **ROC-AUC Score**\n",
        "\n",
        "**ROC (Receiver Operating Characteristic)** curve is a graphical plot that illustrates the diagnostic ability of a binary classifier system as its discrimination threshold (probability threshold to output positive) is varied.\n",
        "\n",
        "- **AUC (Area Under Curve)** is the area under the ROC curve and provides a single value to compare different models (a higher AUC means the model is more robust and its good performance does not vary a lot by changing the threshold).\n",
        "\n",
        "The **True Positive Rate (TPR)** and **False Positive Rate (FPR)** are used to plot the ROC curve:\n",
        "\n",
        "$$\n",
        "\\text{TPR} = \\frac{TP}{TP + FN}, \\quad \\text{FPR} = \\frac{FP}{FP + TN}\n",
        "$$\n",
        "<br>\n",
        "\n",
        "A perfect model has an AUC of 1, while a random classifier has an AUC of 0.5 (assuming binary and balanced classes), because it represents the performance of a **random classifier**. Here's why:\n",
        "\n",
        "##### 1. **Random Guessing**:\n",
        "   - A **random classifier** does not have any discriminative ability; it essentially guesses the class label randomly, with a 50/50 chance for binary classification (assuming balanced classes).\n",
        "   - This means that the classifier will predict positive and negative labels in a completely unstructured manner.\n",
        "\n",
        "##### 2. **ROC Curve for a Random Classifier**:\n",
        "   - A random classifier would have an equal chance of making correct and incorrect predictions, leading to a **True Positive Rate (TPR)** that is proportional to the **False Positive Rate (FPR)**.\n",
        "   - In this case, the ROC curve is a diagonal line from the bottom-left (0, 0) to the top-right (1, 1), indicating that as the threshold changes, the TPR and FPR increase at the same rate. This is essentially a 50% chance of being correct.\n",
        "\n",
        "##### 3. **AUC for a Random Classifier**:\n",
        "   - The **AUC** measures the area under the ROC curve. For a random classifier, the ROC curve is the diagonal line, and the area under this line is exactly **0.5**.\n",
        "   - An AUC of **0.5** implies that the model has no predictive ability; it is effectively guessing. This is the baseline for comparing other models.\n",
        "\n",
        "##### Why Not Below 0.5?\n",
        "- If a classifier has an **AUC below 0.5**, it means the model is worse than random guessing. In theory, a classifier with an AUC less than 0.5 can be inverted (flipping its predictions), and it would perform better than random guessing (yielding an AUC greater than 0.5).\n",
        "- For example, an AUC of 0.3 means that the classifier systematically predicts incorrectly more often than not. You could swap the predictions (i.e., predict positive where the model predicts negative, and vice versa), and you would get an AUC of **1 - 0.3 = 0.7**, which would indicate a better performance.\n",
        "\n",
        "##### AUC and Model Performance:\n",
        "- **AUC = 0.5**: The model has no discriminative ability (random guessing).\n",
        "- **AUC < 0.5**: The model is worse than random guessing and can be improved by inverting predictions.\n",
        "- **AUC > 0.5**: The model is better than random guessing, with higher values indicating better performance.\n",
        "- **AUC = 1.0**: A perfect classifier, with no false positives and no false negatives.\n",
        "\n",
        "In summary, the minimum AUC of **0.5** reflects the performance of a random classifier because it cannot do any better than randomly guessing the outcomes of positive or negative classes. This is the baseline against which the discriminative ability of other models is measured."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 571
        },
        "id": "kXZcW4dwzDUc",
        "outputId": "46a6020a-0ae9-45e3-f13f-ddbdc4459945"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "# Get the predicted probabilities\n",
        "y_pred_prob = model.predict_proba(X_test)[:, 1]  # probabilities for the positive class\n",
        "\n",
        "# Compute ROC curve values: FPR, TPR, and thresholds\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
        "\n",
        "# Compute the Area Under the Curve (AUC)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot the ROC curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')  # Diagonal line representing random guessing\n",
        "\n",
        "# Annotate specific thresholds\n",
        "for i, thr in enumerate(thresholds):\n",
        "    if i % 9 == 0:  # to avoid overcrowding, you can plot every nth threshold\n",
        "        plt.annotate(f'{thr:.2f}', (fpr[i], tpr[i]), textcoords=\"offset points\", xytext=(-10,10), ha='center')\n",
        "\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate', fontsize=14)\n",
        "plt.ylabel('True Positive Rate', fontsize=14)\n",
        "plt.title('ROC Curve with Thresholds', fontsize=16)\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqtx_cVpI-1O"
      },
      "source": [
        "#### 8. **Precision-Recall Curve (PR-curve)**\n",
        "\n",
        "The **Precision-Recall Curve (PR-curve)** is a graphical representation that shows the trade-off between **precision** and **recall** for different threshold values in a classification model, particularly useful when dealing with **imbalanced datasets**. Here’s a detailed explanation:\n",
        "\n",
        "\n",
        "\n",
        "### What the PR-Curve Shows:\n",
        "- The **Precision-Recall Curve** plots **precision** on the Y-axis and **recall** on the X-axis for different threshold values.\n",
        "- As the threshold for classifying an instance as positive changes, the values of precision and recall will change.\n",
        "  - When the threshold is lowered, more instances are classified as positive, leading to higher recall but often lower precision because of more false positives.\n",
        "  - When the threshold is raised, fewer instances are classified as positive, leading to higher precision but lower recall.\n",
        "\n",
        "### Why the PR-Curve is Useful:\n",
        "- **Imbalanced Datasets**: The PR-curve is particularly valuable when dealing with imbalanced datasets where the number of positive instances is much smaller than the negative instances. In such cases, **accuracy** and even the **ROC curve** can be misleading.\n",
        "  - For example, in a dataset where 95% of the data is negative, a classifier can achieve high accuracy by predicting everything as negative. However, the PR-curve focuses on the performance for the minority class (positive class).\n",
        "  \n",
        "- **Trade-off between Precision and Recall**: It shows how well the model performs in balancing precision and recall, which is crucial in scenarios where you need to optimize one over the other. For instance:\n",
        "  - In **spam detection**, you might want higher precision (to avoid labeling legitimate emails as spam).\n",
        "  - In **medical diagnostics**, you might prioritize recall (to minimize false negatives and ensure sick patients are identified).\n",
        "\n",
        "### Interpretation of the PR-Curve:\n",
        "- A good model will maintain high precision while recall increases, meaning the curve will be pushed towards the top-right corner.\n",
        "- A **baseline** model would produce a horizontal line representing the proportion of positive examples in the dataset. If your model’s PR curve is above this line, it is performing better than random guessing.\n",
        "\n",
        "### Comparison with the ROC Curve:\n",
        "- **ROC Curve**: The ROC curve plots **True Positive Rate (TPR)** (recall) against **False Positive Rate (FPR)**, and it’s useful when you care about both classes equally. However, the ROC curve can be misleading when classes are highly imbalanced because it includes FPR, which is not very informative when negatives dominate the dataset.\n",
        "  \n",
        "- **PR Curve**: The PR-curve only focuses on the positive class, and it is more informative than the ROC curve in scenarios where:\n",
        "  - You are working with imbalanced data.\n",
        "  - You care more about the performance of the positive class (e.g., fraud detection, rare disease detection).\n",
        "  \n",
        "### Area Under the PR-Curve (AUC-PR):\n",
        "- Similar to the ROC curve, the **Area Under the Precision-Recall Curve (AUC-PR)** can be used to summarize the model’s performance.\n",
        "  - **AUC-PR = 1.0**: Perfect classifier (high precision and recall at all thresholds).\n",
        "  - **AUC-PR near 0**: Poor classifier.\n",
        "  - **AUC-PR closer to the baseline**: Indicates a model that is close to random guessing.\n",
        "\n",
        "### Example:\n",
        "In a dataset where you are detecting fraudulent transactions (rare positives), the PR curve can tell you how much you will sacrifice in precision as you increase recall. This trade-off is critical because in such a case, you may want to balance precision and recall differently depending on the application (e.g., minimizing false positives vs. catching as many fraud cases as possible)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 571
        },
        "id": "XWq2YKIiI9v0",
        "outputId": "34c81213-eebc-4b9f-8d92-849500bfc828"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_recall_curve\n",
        "\n",
        "# Calculate precision, recall, and thresholds for PR curve\n",
        "precision, recall, pr_thresholds = precision_recall_curve(y_test, y_pred_prob)\n",
        "\n",
        "# Plotting the PR curve with threshold annotations\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(recall, precision, color='blue', lw=2, label='PR curve')\n",
        "\n",
        "# Annotate specific thresholds on the PR curve\n",
        "for i, thr in enumerate(pr_thresholds):\n",
        "    if i % 20 == 0:  # to avoid overcrowding, annotate every nth threshold\n",
        "        plt.annotate(f'{thr:.2f}', (recall[i], precision[i]), textcoords=\"offset points\", xytext=(-10,10), ha='center')\n",
        "\n",
        "plt.xlabel('Recall', fontsize=14)\n",
        "plt.ylabel('Precision', fontsize=14)\n",
        "plt.title('Precision-Recall Curve with Thresholds', fontsize=16)\n",
        "plt.legend(loc=\"lower left\")\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gu53kyazYXLq"
      },
      "source": [
        "### Question: What is the Baseline in a PR Curve?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-TjPn0cT-FE"
      },
      "source": [
        "### What is Log-Loss?\n",
        "\n",
        "**Log-Loss**, also known as **Logarithmic Loss** or **Binary Cross-Entropy**, is a performance metric that evaluates the accuracy of probabilistic predictions for a binary classification model. Unlike simpler metrics like accuracy, log-loss takes into account the predicted probability of each class and penalizes incorrect classifications more harshly.\n",
        "\n",
        "The formula for log-loss is:\n",
        "\n",
        "$$\n",
        "\\text{Log-Loss} = - \\frac{1}{n} \\sum_{i=1}^{n} \\left[ y_i \\log(p_i) + (1 - y_i) \\log(1 - p_i) \\right]\n",
        "$$\n",
        "\n",
        "Where:\n",
        "- $(n)$ is the number of data points.\n",
        "- $(y_i)$ is the actual label (0 or 1).\n",
        "- $(p_i)$ is the predicted probability for the positive class (between 0 and 1).\n",
        "\n",
        "### Key Points:\n",
        "- **Log-Loss** measures how well a model’s predicted probabilities match the actual outcomes.\n",
        "- **Lower log-loss** values are better, as it indicates that the predicted probabilities are closer to the true labels.\n",
        "  - **Log-Loss = 0**: Perfect predictions (the model predicts 1 with 100% certainty when the true label is 1, and similarly for 0).\n",
        "  - **Log-Loss approaches infinity**: The model is confidently wrong (e.g., predicting a probability of 1 when the true label is 0).\n",
        "\n",
        "### Why Log-Loss is Important:\n",
        "\n",
        "1. **Probabilistic Predictions**:\n",
        "   - Unlike accuracy, precision, or recall, which only evaluate hard class predictions (0 or 1), log-loss evaluates the **quality of the predicted probabilities**.\n",
        "   - In many real-world applications, knowing the probability of an event is just as important as the predicted class. For example, in medical diagnostics or financial risk assessments, you may want to understand the confidence level of predictions (e.g., 80% chance of a disease vs. 55% chance).\n",
        "\n",
        "2. **Capturing Confidence**:\n",
        "   - Log-loss penalizes **overconfident wrong predictions** much more heavily than predictions that are closer to the correct probability. For example:\n",
        "     - Predicting 0.99 when the true label is 1 leads to a small penalty.\n",
        "     - Predicting 0.01 when the true label is 1 leads to a much larger penalty.\n",
        "   - This characteristic encourages models to be cautious and avoid making extremely confident incorrect predictions.\n",
        "\n",
        "3. **Handling Imbalanced Classes**:\n",
        "   - In **imbalanced datasets**, accuracy can be misleading because a model might perform well by always predicting the majority class. However, log-loss takes into account the confidence of the model's predictions, providing a more nuanced assessment of performance, especially for the minority class.\n",
        "\n",
        "4. **Continuous Evaluation**:\n",
        "   - Since log-loss evaluates the predicted probabilities rather than the final binary classification, it provides more detailed feedback during model training. It can guide model tuning by revealing how confident and correct the model is on average.\n",
        "\n",
        "5. **Penalizes Misclassification**:\n",
        "   - Log-loss penalizes false predictions more harshly than correct predictions. If a model is unsure about a prediction (e.g., predicting a probability close to 0.5), the penalty is less severe compared to confidently making an incorrect prediction.\n",
        "\n",
        "### Example:\n",
        "Consider a binary classification task, and you have the following predictions:\n",
        "\n",
        "| Actual | Predicted Probability |\n",
        "|--------|-----------------------|\n",
        "| 1      | 0.9                   |\n",
        "| 0      | 0.2                   |\n",
        "| 1      | 0.6                   |\n",
        "| 0      | 0.8                   |\n",
        "\n",
        "- For the first prediction, the model is confident (0.9) and correct, so the log-loss will be small.\n",
        "- For the last prediction, the model is confident but wrong (0.8 when the true label is 0), so the log-loss will be large.\n",
        "\n",
        "This shows how log-loss differentiates between predictions that are confident and wrong versus predictions that are uncertain.\n",
        "\n",
        "### Why is Log-Loss Preferable in Some Cases?\n",
        "\n",
        "- **Probability-Sensitive**: Log-loss cares about how confident the model is in its predictions, which is crucial when working with probabilistic predictions. For instance, if you want your model to provide insights into how likely a certain event is (e.g., fraud detection, medical diagnosis), log-loss will penalize overconfidence in wrong predictions.\n",
        "  \n",
        "- **Model Calibration**: Log-loss helps in assessing how well-calibrated a model is. A well-calibrated model will output probabilities that reflect the true likelihood of events. For example, if a model predicts a 0.7 probability for 100 events, about 70 of those events should be positive.\n",
        "\n",
        "### When to Use Log-Loss:\n",
        "\n",
        "- **Classification problems with imbalanced data** where you need to account for the confidence of predictions.\n",
        "- **Applications requiring probabilistic predictions** rather than hard classifications, such as financial risk models, medical diagnosis, or weather prediction.\n",
        "- **When you need a metric that penalizes confident misclassifications**, which encourages models to be careful when predicting probabilities.\n",
        "\n",
        "In summary, **log-loss** is important because it evaluates the quality of probabilistic predictions and penalizes overconfident wrong predictions, providing a more nuanced and useful measure for tasks where understanding prediction probabilities is critical."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23RZaW_oMi5o",
        "outputId": "77085c80-9d6d-410b-bf6a-349543b6754a"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import log_loss\n",
        "\n",
        "# Calculate log-loss\n",
        "log_loss_value = log_loss(y_test, y_pred_prob)\n",
        "\n",
        "log_loss_value\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzzC9iS7np5v"
      },
      "source": [
        "# (For You!) Imbalanced Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "jrmesFfXPNG7",
        "outputId": "ac0a4ddf-2ab2-4ca5-be34-948340de992f"
      },
      "outputs": [],
      "source": [
        "# Create an imbalanced synthetic binary classification dataset (positive is minority)\n",
        "X_imbalanced, y_imbalanced = make_classification(n_samples=1000, n_features=20, n_classes=2,\n",
        "                                                 n_clusters_per_class=1, weights=[0.95, 0.05],\n",
        "                                                 flip_y=0, random_state=42)\n",
        "\n",
        "# Split the dataset into training and test sets\n",
        "X_train_imbalanced, X_test_imbalanced, y_train_imbalanced, y_test_imbalanced = train_test_split(\n",
        "    X_imbalanced, y_imbalanced, test_size=0.3, random_state=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDp8MBjwqKZB"
      },
      "source": [
        "## What happened?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfOkygy7Hjpd"
      },
      "source": [
        "\n",
        "# **Classification Metrics (Multiclass)**\n",
        "\n",
        "### 1. **Confusion Matrix (Multiclass)**\n",
        "\n",
        "In a **multiclass confusion matrix**, the rows represent the actual classes, and the columns represent the predicted classes. Instead of just two classes (positive and negative), each class has its own row and column.\n",
        "\n",
        "|                   | **Predicted Class 1** | **Predicted Class 2** | **...** | **Predicted Class N** |\n",
        "|-------------------|-----------------------|-----------------------|--------|-----------------------|\n",
        "| **True Class 1**   | **TP for Class 1**    | **FP for Class 1**     | ...    |                       |\n",
        "| **True Class 2**   | **FN for Class 2**    | **TP for Class 2**     | ...    |                       |\n",
        "| **...**           |                       |                       | ...    |                       |\n",
        "| **True Class N**   |                       |                       |        | **TP for Class N**    |\n",
        "\n",
        "- **True Positives (TP)**: Correctly predicted instances for each class.\n",
        "- **False Positives (FP)**: Instances incorrectly predicted as a particular class.\n",
        "- **False Negatives (FN)**: Instances of a class that were incorrectly predicted as another class.\n",
        "- **True Negatives (TN)** are generally calculated for each class by summing all the instances not in that class.\n",
        "\n",
        "This matrix helps in calculating other multiclass metrics like precision, recall, F1-score, and accuracy.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **Accuracy (Multiclass)**\n",
        "\n",
        "In the multiclass case, **accuracy** still measures the proportion of correct predictions out of all predictions, regardless of the class.\n",
        "\n",
        "$$\n",
        "\\text{Accuracy} = \\frac{\\text{Total Correct Predictions}}{\\text{Total Predictions}}\n",
        "$$\n",
        "\n",
        "Where **Total Correct Predictions** is the sum of the diagonal elements of the confusion matrix (all true positives).\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **Precision (Multiclass)**\n",
        "\n",
        "For multiclass problems, **precision** can be calculated in three ways:\n",
        "\n",
        "- **Macro-averaged Precision**: Precision is calculated for each class separately and then averaged equally, treating all classes equally.\n",
        "  \n",
        "  $$\n",
        "  \\text{Macro Precision} = \\frac{1}{N} \\sum_{i=1}^{N} \\frac{TP_i}{TP_i + FP_i}\n",
        "  $$\n",
        "\n",
        "- **Weighted Precision**: Precision is calculated for each class and then weighted by the number of true instances in each class to account for class imbalance.\n",
        "  \n",
        "  $$\n",
        "  \\text{Weighted Precision} = \\sum_{i=1}^{N} \\frac{n_i}{n} \\cdot \\frac{TP_i}{TP_i + FP_i}\n",
        "  $$\n",
        "\n",
        "- **Micro-averaged Precision**: Treats all classes as a single binary classification problem by summing the **TP**, **FP**, and **FN** across all classes:\n",
        "  \n",
        "  $$\n",
        "  \\text{Micro Precision} = \\frac{\\sum TP}{\\sum (TP + FP)}\n",
        "  $$\n",
        "\n",
        "---\n",
        "\n",
        "### 4. **Recall (Sensitivity or True Positive Rate) (Multiclass)**\n",
        "\n",
        "Similarly, recall can be computed in three ways:\n",
        "\n",
        "- **Macro-averaged Recall**: Calculated by averaging the recall for each class equally:\n",
        "  \n",
        "  $$\n",
        "  \\text{Macro Recall} = \\frac{1}{N} \\sum_{i=1}^{N} \\frac{TP_i}{TP_i + FN_i}\n",
        "  $$\n",
        "\n",
        "- **Weighted Recall**: Adjusts for the number of true instances in each class:\n",
        "  \n",
        "  $$\n",
        "  \\text{Weighted Recall} = \\sum_{i=1}^{N} \\frac{n_i}{n} \\cdot \\frac{TP_i}{TP_i + FN_i}\n",
        "  $$\n",
        "\n",
        "- **Micro-averaged Recall**: Treats all classes as a single binary classification problem by summing the **TP**, **FP**, and **FN** across all classes:\n",
        "  \n",
        "  $$\n",
        "  \\text{Micro Recall} = \\frac{\\sum TP}{\\sum (TP + FN)}\n",
        "  $$\n",
        "\n",
        "---\n",
        "\n",
        "### 5. **Specificity (True Negative Rate) (Multiclass)**\n",
        "\n",
        "For multiclass classification, **specificity** is calculated for each class by treating it as a binary classification (this class vs. all others):\n",
        "\n",
        "$$\n",
        "\\text{Specificity for Class i} = \\frac{TN_i}{TN_i + FP_i}\n",
        "$$\n",
        "\n",
        "Similar to precision and recall, you can compute **macro-averaged specificity** (equal weight for each class) or **weighted specificity** (weighted by class size).\n",
        "\n",
        "---\n",
        "\n",
        "### 6. **Balanced Accuracy (Multiclass)**\n",
        "\n",
        "**Balanced Accuracy** is particularly useful for imbalanced datasets in multiclass problems. It is calculated by averaging the recall (sensitivity) for each class, giving equal importance to each class, regardless of class size.\n",
        "\n",
        "#### **Unweighted Balanced Accuracy**:\n",
        "\n",
        "$$\n",
        "\\text{Balanced Accuracy} = \\frac{1}{N} \\sum_{i=1}^{N} \\frac{TP_i}{TP_i + FN_i}\n",
        "$$\n",
        "\n",
        "Where:\n",
        "- $ N $ is the total number of classes.\n",
        "- $ TP_i $ is the number of true positives for class $ i $.\n",
        "- $ FN_i $ is the number of false negatives for class $ i $.\n",
        "\n",
        "---\n",
        "\n",
        "#### **Weighted Balanced Accuracy**:\n",
        "\n",
        "\n",
        "$$ {Weighted Balanced Accuracy} = \\sum_{i=1}^{N} w_i \\cdot \\frac{TP_i}{TP_i + FN_i}\n",
        "$$\n",
        "\n",
        "Where:\n",
        "- $ w_i $ is the weight for class $ i $, which is proportional to the number of true samples in class $ i $:\n",
        "  $$\n",
        "  w_i = \\frac{n_i}{n}\n",
        "  $$\n",
        "  Where $ n_i $ is the number of true instances of class $ i $, and $ n $ is the total number of instances across all classes.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### 7. **F1-Score (Multiclass)**\n",
        "\n",
        "The **F1-Score** can be computed in three ways:\n",
        "\n",
        "- **Macro F1-Score**: The F1-score is calculated for each class separately, and the results are averaged equally.\n",
        "  \n",
        "  $$\n",
        "  \\text{Macro F1} = \\frac{1}{N} \\sum_{i=1}^{N} \\frac{2 \\cdot \\text{Precision}_i \\cdot \\text{Recall}_i}{\\text{Precision}_i + \\text{Recall}_i}\n",
        "  $$\n",
        "\n",
        "- **Weighted F1-Score**: Similar to precision and recall, it weighs the F1 score for each class based on the number of true instances in that class:\n",
        "  \n",
        "  $$\n",
        "  \\text{Weighted F1} = \\sum_{i=1}^{N} \\frac{n_i}{n} \\cdot \\frac{2 \\cdot \\text{Precision}_i \\cdot \\text{Recall}_i}{\\text{Precision}_i + \\text{Recall}_i}\n",
        "  $$\n",
        "\n",
        "- **Micro-averaged F1-Score**: In this case, micro-averaged precision and recall are used to compute a single F1-score across all classes. Since micro-averaging sums up the **TP**, **FP**, and **FN** globally, the **micro F1** is also computed globally:\n",
        "  \n",
        "  $$\n",
        "  \\text{Micro F1} = \\frac{2 \\times \\text{Micro Precision} \\times \\text{Micro Recall}}{\\text{Micro Precision} + \\text{Micro Recall}}\n",
        "  $$\n",
        "\n",
        "---\n",
        "\n",
        "### Summary of Averages:\n",
        "\n",
        "- **Macro**: Treats each class equally and averages metrics across classes.\n",
        "- **Weighted**: Weighs each class's contribution based on its size.\n",
        "- **Micro**: Sums up the true positives, false positives, and false negatives globally across all classes and treats the problem as a single binary classification.\n",
        "\n",
        "These approaches provide flexibility when evaluating multiclass classifiers, allowing you to emphasize overall performance (micro), treat all classes equally (macro), or account for class imbalance (weighted).\n",
        "\n",
        "### 8. **ROC-AUC for Multiclass**\n",
        "\n",
        "In the multiclass setting, **ROC-AUC** can be computed by considering each class against all other classes (one-vs-rest approach). For each class $i$, we compute the ROC curve, treating class $i$ as the positive class and all others as the negative class.\n",
        "\n",
        "The overall **multiclass AUC** can then be calculated by averaging the AUC scores for all classes.\n",
        "\n",
        "For each class $i$, the **True Positive Rate (TPR)** and **False Positive Rate (FPR)** are defined as:\n",
        "\n",
        "$$\n",
        "\\text{TPR}_i = \\frac{TP_i}{TP_i + FN_i}, \\quad \\text{FPR}_i = \\frac{FP_i}{FP_i + TN_i}\n",
        "$$\n",
        "\n",
        "The **ROC-AUC** is then computed as the area under the ROC curve for each class, and the average AUC can be taken across all classes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MLhXwKpjC6C"
      },
      "source": [
        "# Multi-class Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 948
        },
        "id": "rhca0r5tfq8L",
        "outputId": "52357539-ee56-41f8-bae9-b192a3db054b"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC  # Support Vector Classifier\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_recall_fscore_support\n",
        "from sklearn.metrics import roc_curve, auc, balanced_accuracy_score\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import label_binarize\n",
        "import pandas as pd\n",
        "\n",
        "# Step 1: Create synthetic multiclass dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_classes=8, n_informative=5, n_clusters_per_class=1, random_state=42)\n",
        "\n",
        "# Step 2: Split the data into 70% train and 30% test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Cross Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "jk9gL-zOkmu8",
        "outputId": "3ea90a23-c2b1-4d8d-a877-60801c01df39"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_classification\n",
        "from sklearn.svm import SVC  # Support Vector Classifier\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_recall_fscore_support\n",
        "from sklearn.metrics import roc_curve, auc, balanced_accuracy_score\n",
        "from sklearn.model_selection import KFold\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import label_binarize\n",
        "import pandas as pd\n",
        "\n",
        "# Create synthetic multiclass dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_classes=5, n_informative=5, n_clusters_per_class=2, random_state=42)\n",
        "\n",
        "# Initialize regular cross-validation (not stratified)\n",
        "n_classes = len(np.unique(y))\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Variables to accumulate results across folds\n",
        "all_conf_matrices = []\n",
        "all_roc_auc = []\n",
        "precision_scores = []\n",
        "recall_scores = []\n",
        "f1_scores = []\n",
        "accuracy_scores = []\n",
        "balanced_acc_scores = []\n",
        "\n",
        "# Cross-validation loop\n",
        "for train_index, test_index in kf.split(X, y):\n",
        "    # Split the data\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "    # Train the model\n",
        "    model = SVC(probability=True, random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Predict\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Accuracy\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    accuracy_scores.append(accuracy)\n",
        "\n",
        "    # Precision, Recall, F1 (Macro)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='macro')\n",
        "    precision_scores.append(precision)\n",
        "    recall_scores.append(recall)\n",
        "    f1_scores.append(f1)\n",
        "\n",
        "    # Balanced Accuracy\n",
        "    balanced_acc = balanced_accuracy_score(y_test, y_pred)\n",
        "    balanced_acc_scores.append(balanced_acc)\n",
        "\n",
        "    # ROC Curve and AUC\n",
        "    y_test_bin = label_binarize(y_test, classes=np.arange(n_classes))\n",
        "    y_pred_prob = model.predict_proba(X_test)\n",
        "\n",
        "# Aggregate Results\n",
        "avg_accuracy = np.mean(accuracy_scores)\n",
        "avg_precision = np.mean(precision_scores)\n",
        "avg_recall = np.mean(recall_scores)\n",
        "avg_f1 = np.mean(f1_scores)\n",
        "avg_balanced_acc = np.mean(balanced_acc_scores)\n",
        "\n",
        "std_accuracy = np.std(accuracy_scores)\n",
        "std_precision = np.std(precision_scores)\n",
        "std_recall = np.std(recall_scores)\n",
        "std_f1 = np.std(f1_scores)\n",
        "std_balanced_acc = np.std(balanced_acc_scores)\n",
        "\n",
        "# Display aggregated metrics with mean and standard deviation in formatted style\n",
        "metrics_df = {\n",
        "    'Metric': ['Accuracy', 'Precision (Macro)', 'Recall (Macro)', 'F1 (Macro)', 'Balanced Accuracy'],\n",
        "    'Score': [f\"{(avg_accuracy * 100):.1f}% ± {(std_accuracy * 100):.1f}%\",\n",
        "              f\"{(avg_precision * 100):.1f}% ± {(std_precision * 100):.1f}%\",\n",
        "              f\"{(avg_recall * 100):.1f}% ± {(std_recall * 100):.1f}%\",\n",
        "              f\"{(avg_f1 * 100):.1f}% ± {(std_f1 * 100):.1f}%\",\n",
        "              f\"{(avg_balanced_acc * 100):.1f}% ± {(std_balanced_acc * 100):.1f}%\"]\n",
        "}\n",
        "\n",
        "# Convert to DataFrame\n",
        "df_metrics = pd.DataFrame(metrics_df)\n",
        "\n",
        "df_metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ChT3kpKjjPw-"
      },
      "source": [
        "# Excercise!\n",
        "\n",
        "Make a synthetic **imbalanced**, **multi-class**, dataset. <br>\n",
        "Perform cross validation and see the results (micro, macro, weighted). <br>\n",
        "What could go wrong in this scenario?\n",
        "\n",
        "- Check stratified cross validation and once again do it with stratified CV."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Rbsf9sfSjRS6"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
