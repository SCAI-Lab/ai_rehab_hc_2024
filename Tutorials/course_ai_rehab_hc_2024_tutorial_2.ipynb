{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gs7XhT8U1GvI"
      },
      "source": [
        "### Introduction to Popular Machine Learning Libraries\n",
        "\n",
        "Several powerful libraries are widely used for tasks like data preprocessing, model training, and evaluation. Some of the most popular machine learning libraries include:\n",
        "\n",
        "#### 1. **NumPy** (introduced in tutorial 1)\n",
        "   - **Purpose**: Provides support for large, multi-dimensional arrays and matrices, along with a collection of mathematical functions to operate on these arrays.\n",
        "   - **Use Case**: NumPy is the foundation for many other libraries and is used for efficient numerical computation, matrix manipulation, and linear algebra operations.\n",
        "\n",
        "#### 2. **Pandas** (introduced in tutorial 1)\n",
        "   - **Purpose**: A library designed for data manipulation and analysis. It provides data structures like DataFrames, which are useful for handling structured datasets (_Remember! Pandas is the Excel in Python!_)\n",
        "   - **Use Case**: Pandas is commonly used for data cleaning, preparation, and exploratory data analysis (EDA).\n",
        "\n",
        "#### 3. **Matplotlib and Seaborn** (introduced in tutorial 1)\n",
        "   - **Purpose**: These libraries are used for data visualization. Matplotlib provides a flexible platform for creating static, animated, and interactive plots, while Seaborn simplifies the creation of informative and attractive statistical graphics.\n",
        "   - **Use Case**: Used extensively for visualizing data distributions, trends, and relationships between features.\n",
        "\n",
        "#### 4. **scikit-learn**\n",
        "   - **Purpose**: A comprehensive library that provides simple and efficient tools for data mining and machine learning. It includes algorithms for classification, regression, clustering, dimensionality reduction, and more.\n",
        "   - **Use Case**: Scikit-learn is the go-to library for traditional machine learning tasks such as training classifiers, regression models, clustering algorithms, and for performing cross-validation and model evaluation.\n",
        "\n",
        "#### 5. **XGBoost**\n",
        "   - **Purpose**: An optimized gradient boosting library designed for speed and performance. XGBoost is highly efficient for both classification and regression problems and is often used in Kaggle competitions.\n",
        "   - **Use Case**: Best suited for structured/tabular data. It is widely used for feature engineering, handling missing data, and improving predictive accuracy.\n",
        "\n",
        "#### 6. **CatBoost**\n",
        "   - **Purpose**: A machine learning library that is particularly good for handling categorical features. It provides excellent performance in classification and regression tasks.\n",
        "   - **Use Case**: Best used when working with categorical data. It’s known for its ability to handle categorical features automatically without the need for preprocessing steps like one-hot encoding.\n",
        "\n",
        "#### 7. **LightGBM**\n",
        "   - **Purpose**: A gradient boosting framework that uses tree-based learning algorithms. LightGBM is optimized for fast training on large datasets and is often compared with XGBoost.\n",
        "   - **Use Case**: Ideal for large datasets where high computational efficiency is required. It is particularly useful in machine learning competitions due to its speed and accuracy.\n",
        "\n",
        "#### 8. **Keras**\n",
        "   - **Purpose**: A high-level neural network API, written in Python and capable of running on top of TensorFlow, PyTorch, Theano, or CNTK. It allows for quick prototyping of deep learning models.\n",
        "   - **Use Case**: Used for designing and training neural networks with minimal code. Keras is highly user-friendly and supports both convolutional and recurrent networks.\n",
        "\n",
        "#### 9. **TensorFlow**\n",
        "   - **Purpose**: A powerful open-source deep learning library developed by Google. TensorFlow supports large-scale machine learning models, especially neural networks, and is widely used for both research and production environments.\n",
        "   - **Use Case**: Primarily used for deep learning tasks, such as training deep neural networks (e.g., convolutional and recurrent networks), natural language processing (NLP), and computer vision.\n",
        "\n",
        "#### 10. **PyTorch**\n",
        "   - **Purpose**: Another open-source deep learning library, developed by Facebook. PyTorch emphasizes flexibility and ease of use, making it popular in the research community.\n",
        "   - **Use Case**: Similar to TensorFlow, PyTorch is used for deep learning tasks like image classification, object detection, and sequence modeling, with a particular strength in dynamic computational graphs and debugging.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aw2g4o8UyfpK"
      },
      "source": [
        "### Introduction to Scikit-learn\n",
        "\n",
        "`scikit-learn` is a powerful, open-source machine learning library for Python. It provides simple and efficient tools for data analysis and machine learning, built on top of popular Python libraries like `NumPy`, `SciPy`, and `matplotlib`(briefly introduced in tutorial 1). It is widely used for various machine learning tasks, ranging from simple data preprocessing to advanced model training and evaluation.\n",
        "\n",
        "### Key Features of Scikit-learn\n",
        "\n",
        "Scikit-learn offers a wide variety of tools and algorithms for the following tasks:\n",
        "\n",
        "#### 1. **Supervised Learning**\n",
        "   - **Classification**: Predict categorical labels (e.g., yes/no, spam/ham) using models like:\n",
        "     - `Logistic Regression`\n",
        "     - `Support Vector Machines (SVM)`\n",
        "     - `k-Nearest Neighbors (k-NN)`\n",
        "     - `Random Forest`\n",
        "     - `Decision Trees`\n",
        "     - `Naive Bayes`\n",
        "   - **Regression**: Predict continuous target variables (e.g., house prices, stock values) with models such as:\n",
        "     - `Linear Regression`\n",
        "     - `Ridge Regression`\n",
        "     - `Lasso`\n",
        "     - `Support Vector Regression (SVR)`\n",
        "     - `Random Forest Regressor`\n",
        "\n",
        "#### 2. **Unsupervised Learning**\n",
        "   - **Clustering**: Group similar data points into clusters without predefined labels using algorithms like:\n",
        "     - `k-Means`\n",
        "     - `Hierarchical Clustering`\n",
        "     - `DBSCAN`\n",
        "     - `Mean-Shift`\n",
        "   - **Dimensionality Reduction**: Reduce the number of features in a dataset while preserving important information. Techniques include:\n",
        "     - `Principal Component Analysis (PCA)`\n",
        "     - `t-distributed Stochastic Neighbor Embedding (t-SNE)`\n",
        "     - `Truncated SVD`\n",
        "\n",
        "#### 3. **Model Selection and Evaluation**\n",
        "   - **Cross-Validation**: Evaluate model performance using techniques like `K-fold Cross Validation`.\n",
        "   - **Grid Search/Randomized Search**: Perform hyperparameter tuning to find the best model configuration.\n",
        "   - **Metrics**: Scikit-learn provides a variety of evaluation metrics, such as:\n",
        "     - `Accuracy`, `Precision`, `Recall`, `F1-Score` for classification.\n",
        "     - `Mean Squared Error (MSE)` and `R²` for regression.\n",
        "     - `Silhouette Score` for clustering.\n",
        "\n",
        "#### 4. **Preprocessing and Feature Engineering**\n",
        "   - **Data Standardization/Normalization**: Transform data for better model performance using:\n",
        "     - `StandardScaler`, `MinMaxScaler`\n",
        "   - **Handling Missing Values**: Impute missing values using `SimpleImputer`.\n",
        "   - **Encoding Categorical Variables**: Convert categorical variables into numeric representations using:\n",
        "     - `LabelEncoder`\n",
        "     - `OneHotEncoder`\n",
        "   - **Feature Selection**: Select the most important features using techniques like `SelectKBest`, `Recursive Feature Elimination (RFE)`.\n",
        "\n",
        "#### 5. **Ensemble Methods**\n",
        "   - Combine multiple models to improve performance. Scikit-learn provides:\n",
        "     - `Random Forest`\n",
        "     - `Gradient Boosting`\n",
        "     - `AdaBoost`\n",
        "     - `VotingClassifier` and `VotingRegressor`\n",
        "\n",
        "#### 6. **Pipelines**\n",
        "   - Create pipelines that combine multiple preprocessing steps and model training into a single workflow, making your code cleaner and more reproducible.\n",
        "\n",
        "#### 7. **Model Persistence**\n",
        "   - Save and load trained models using `joblib` or `pickle`, allowing you to reuse models without retraining them.\n",
        "\n",
        "---\n",
        "\n",
        "### Example:\n",
        "\n",
        "```python\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Random Forest classifier\n",
        "model = RandomForestClassifier()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions and evaluate the model\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "`scikit-learn` is versatile and highly efficient, making it a go-to library for machine learning tasks in both research and production environments.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVuAF6IF5VFD"
      },
      "source": [
        "### Datasets\n",
        "\n",
        "Scikit-learn provides a variety of built-in datasets that are useful for practicing machine learning tasks, including classificationand regression. Here are some of the most commonly used datasets in scikit-learn:\n",
        "\n",
        "Here are more detailed descriptions of the datasets, including the number of features, samples, and other useful information:\n",
        "\n",
        "### 1. **Iris Dataset**\n",
        "   - **Description**: A classic dataset for multiclass classification of iris flowers into three species: setosa, versicolor, and virginica. It contains 150 samples with four numeric features: sepal length, sepal width, petal length, and petal width.\n",
        "   - **Number of Samples**: 150\n",
        "   - **Number of Features**: 4 (sepal length, sepal width, petal length, petal width)\n",
        "   - **Classes**: 3 (Setosa, Versicolor, Virginica)\n",
        "   \n",
        "   ```python\n",
        "   from sklearn.datasets import load_iris\n",
        "   iris = load_iris()\n",
        "   features = iris.data\n",
        "   outputs = iris.target\n",
        "   ```\n",
        "\n",
        "### 2. **Wine Dataset**\n",
        "   - **Description**: This dataset contains the results of chemical analysis of wines grown in the same region in Italy, derived from three different cultivars. There are 13 features that describe different chemical properties of each wine.\n",
        "   - **Number of Samples**: 178\n",
        "   - **Number of Features**: 13 (alcohol, malic acid, ash, alcalinity of ash, magnesium, total phenols, flavanoids, etc.)\n",
        "   - **Classes**: 3 (three different wine cultivars)\n",
        "   \n",
        "   ```python\n",
        "   from sklearn.datasets import load_wine\n",
        "   wine = load_wine()\n",
        "   ```\n",
        "\n",
        "### 3. **Breast Cancer Dataset**\n",
        "   - **Description**: A binary classification dataset used to predict whether breast cancer is benign or malignant based on 30 features derived from a digitized image of a fine needle aspirate (FNA) of a breast mass.\n",
        "   - **Number of Samples**: 569\n",
        "   - **Number of Features**: 30 (mean radius, mean texture, mean perimeter, mean area, mean smoothness, etc.)\n",
        "   - **Classes**: 2 (benign, malignant)\n",
        "   \n",
        "   ```python\n",
        "   from sklearn.datasets import load_breast_cancer\n",
        "   cancer = load_breast_cancer()\n",
        "   ```\n",
        "\n",
        "### 4. **Digits Dataset**\n",
        "   - **Description**: A multiclass classification dataset that consists of 8x8 pixel images of handwritten digits (0–9). The goal is to classify these images into the correct digit.\n",
        "   - **Number of Samples**: 1797\n",
        "   - **Number of Features**: 64 (8x8 pixel values per image, each feature represents the grayscale value of a pixel)\n",
        "   - **Classes**: 10 (digits 0–9)\n",
        "   \n",
        "   ```python\n",
        "   from sklearn.datasets import load_digits\n",
        "   digits = load_digits()\n",
        "   ```\n",
        "\n",
        "### 5. **Labeled Faces in the Wild (LFW) Dataset**\n",
        "   - **Description**: This dataset contains images of famous people and is used for face recognition tasks. Each image is represented as a 1850-dimensional feature vector (after resizing).\n",
        "   - **Number of Samples**: 13,233\n",
        "   - **Number of Features**: 1850 (resized 40x30 grayscale image)\n",
        "   - **Classes**: 5749 (individuals)\n",
        "   \n",
        "   ```python\n",
        "   from sklearn.datasets import fetch_lfw_people\n",
        "   lfw_people = fetch_lfw_people(min_faces_per_person=70, resize=0.4)\n",
        "   ```\n",
        "\n",
        "### 6. **Diabetes Dataset**\n",
        "   - **Description**: A dataset related to diabetes progression, primarily used for regression tasks. It includes 10 features related to age, sex, BMI, blood pressure, and other measurements.\n",
        "   - **Number of Samples**: 442\n",
        "   - **Number of Features**: 10 (age, sex, body mass index, blood pressure, and six blood serum measurements)\n",
        "   - **Target**: Continuous variable (progression of diabetes)\n",
        "   \n",
        "   ```python\n",
        "   from sklearn.datasets import load_diabetes\n",
        "   diabetes = load_diabetes()  # Typically used for regression\n",
        "   ```\n",
        "\n",
        "### 7. **California Housing Dataset** (for Binary Classification)\n",
        "   - **Description**: While primarily used for regression tasks, this dataset can be converted into a classification problem by binarizing the target variable. It contains housing data from districts in California, with features like median income, average house age, and proximity to the ocean.\n",
        "   - **Number of Samples**: 20,640\n",
        "   - **Number of Features**: 8 (median income, house age, latitude, longitude, number of rooms, etc.)\n",
        "   - **Target**: Median house value (continuous, can be binarized for classification)\n",
        "   \n",
        "   ```python\n",
        "   from sklearn.datasets import fetch_california_housing\n",
        "   housing = fetch_california_housing()  # Typically used for regression\n",
        "   ```\n",
        "\n",
        "### 8. **Covertype Dataset**\n",
        "   - **Description**: A large multiclass classification dataset used to predict forest cover types based on cartographic data. It contains 54 features derived from elevation, aspect, slope, and other geographical properties.\n",
        "   - **Number of Samples**: 581,012\n",
        "   - **Number of Features**: 54 (elevation, aspect, slope, horizontal distance to hydrology, etc.)\n",
        "   - **Classes**: 7 (forest cover types)\n",
        "   \n",
        "   ```python\n",
        "   from sklearn.datasets import fetch_covtype\n",
        "   covtype = fetch_covtype()\n",
        "   ```\n",
        "\n",
        "### 9. **RCV1 (Reuters Corpus Volume I) Dataset**\n",
        "   - **Description**: A large dataset for text classification consisting of over 800,000 manually categorized newswire stories. Each story is labeled with multiple categories.\n",
        "   - **Number of Samples**: 804,414\n",
        "   - **Number of Features**: 47236\n",
        "   - **Classes**: 103 (topics/categories)\n",
        "   \n",
        "   ```python\n",
        "   from sklearn.datasets import fetch_rcv1\n",
        "   rcv1 = fetch_rcv1()\n",
        "   ```\n",
        "\n",
        "### 10. **KDD Cup 99 Dataset**\n",
        "   - **Description**: A dataset used for network intrusion detection and classification. It contains various network traffic features to classify normal and attack types in network connections.\n",
        "   - **Number of Samples**: 494,021\n",
        "   - **Number of Features**: 41 (duration, protocol type, service, flag, source bytes, etc.)\n",
        "   - **Classes**: 23 (normal and 22 types of network attacks)\n",
        "   \n",
        "   ```python\n",
        "   from sklearn.datasets import fetch_kddcup99\n",
        "   kddcup99 = fetch_kddcup99()\n",
        "   ```\n",
        "\n",
        "### 11. **Spam Base Dataset** (from OpenML)\n",
        "   - **Description**: A dataset used for binary classification to determine whether an email is spam or not spam, based on word frequency and other content-based features.\n",
        "   - **Number of Samples**: 4,601\n",
        "   - **Number of Features**: 57 (word frequencies, capital letter sequences, etc.)\n",
        "   - **Classes**: 2 (spam, not spam)\n",
        "   \n",
        "   ```python\n",
        "   from sklearn.datasets import fetch_openml\n",
        "   spambase = fetch_openml(name='spambase')\n",
        "   ```\n",
        "\n",
        "---\n",
        "\n",
        "### 12. **Synthetic Data**\n",
        "Creating synthetic data can be a great way to test machine learning algorithms and practice your skills. Scikit-learn provides several utilities to generate synthetic datasets for both classification and regression tasks. Here’s how to create your own synthetic data using scikit-learn:\n",
        "\n",
        "#### **Generating a Synthetic Classification Dataset**\n",
        "Scikit-learn has a function called `make_classification` that allows you to generate a synthetic classification dataset.\n",
        "\n",
        "```python\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# Generate a synthetic dataset for classification\n",
        "X, y = make_classification(n_samples=100,  # Total number of samples\n",
        "                           n_features=2,   # Total number of features\n",
        "                           n_informative=2, # Number of informative features\n",
        "                           n_classes=3,  # Number of classes\n",
        "                           n_redundant=0,   # Number of redundant features\n",
        "                           n_clusters_per_class=1, # Clusters per class\n",
        "                           random_state=42)\n",
        "\n",
        "# Visualize the synthetic dataset\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, edgecolor='k', cmap='Set1')\n",
        "plt.title(\"Synthetic Binary Classification Dataset\")\n",
        "plt.xlabel(\"Feature 1\")\n",
        "plt.ylabel(\"Feature 2\")\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "#### Explanation:\n",
        "- **`n_samples`**: Total number of samples to generate.\n",
        "- **`n_features`**: Total number of features.\n",
        "- **`n_informative`**: Number of features that are informative for the classification task.\n",
        "- **`n_classes`**: The number of classes.\n",
        "- **`n_redundant`**: Number of redundant features.\n",
        "- **`n_clusters_per_class`**: Number of clusters per class.\n",
        "- **`random_state`**: Seed for reproducibility.\n",
        "\n",
        "\n",
        "#### **Generating a Synthetic Regression Dataset**\n",
        "For regression tasks, you can use the `make_regression` function, which generates a synthetic dataset for regression problems.\n",
        "\n",
        "```python\n",
        "from sklearn.datasets import make_regression\n",
        "\n",
        "# Generate a synthetic dataset for regression\n",
        "X_reg, y_reg = make_regression(n_samples=100,  # Total number of samples\n",
        "                               n_features=1,   # Total number of features\n",
        "                               noise=10,       # Standard deviation of the Gaussian noise\n",
        "                               random_state=42)\n",
        "\n",
        "# Visualize the synthetic dataset\n",
        "plt.scatter(X_reg, y_reg, color='blue')\n",
        "plt.title(\"Synthetic Regression Dataset\")\n",
        "plt.xlabel(\"Feature\")\n",
        "plt.ylabel(\"Target\")\n",
        "plt.axhline(0, color='black', lw=0.5, ls='--')  # Add a horizontal line at y=0\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "#### Explanation:\n",
        "- **`n_samples`**: Total number of samples to generate.\n",
        "- **`n_features`**: Total number of features.\n",
        "- **`noise`**: Standard deviation of the Gaussian noise added to the output.\n",
        "- **`random_state`**: Seed for reproducibility.\n",
        "\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "Generating synthetic data is a powerful way to experiment with different machine learning models and understand their behavior. You can easily adjust parameters in `make_classification` and `make_regression` to create datasets that fit your specific needs. Try it yourself!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hmq7BouHk9lA"
      },
      "source": [
        "\n",
        "### **Some Classifiers in Scikit-learn**\n",
        "\n",
        "Scikit-learn offers various classifiers for machine learning tasks. Below are explanations of some of the most commonly used classifiers: **SVM**, **KNN**, **Random Forest**, and **Gradient Boosting**.\n",
        "\n",
        "---\n",
        "\n",
        "### 1. **Support Vector Machine (SVM)**\n",
        "\n",
        "**SVM** is a powerful classifier that works by finding the hyperplane that best separates the classes in feature space. For non-linear data, SVM can use kernels to project the data into a higher-dimensional space where it becomes linearly separable.\n",
        "\n",
        "- **Linear SVM**: Finds a linear decision boundary (hyperplane) to separate the classes.\n",
        "- **Kernel SVM**: Applies a kernel trick (e.g., RBF, polynomial) to handle non-linear separable data by mapping it to a higher dimension.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **K-Nearest Neighbors (KNN)**\n",
        "\n",
        "**KNN** is a simple, intuitive algorithm that classifies a sample based on the majority label of its $ k $-nearest neighbors in the feature space.\n",
        "\n",
        "- **Distance Metric**: Typically uses **Euclidean distance**, but can use others like **Manhattan** or **Minkowski**.\n",
        "- **Voting**: The class of the sample is determined by the majority vote of its nearest neighbors.\n",
        "\n",
        "For example, the distance between two points $ x_i = (x_{i1}, x_{i2}, \\dots, x_{in}) $ and $ x_j = (x_{j1}, x_{j2}, \\dots, x_{jn}) $ in Euclidean space is given by:\n",
        "\n",
        "$$\n",
        "d(x_i, x_j) = \\sqrt{\\sum_{k=1}^{n}(x_{ik} - x_{jk})^2}\n",
        "$$\n",
        "\n",
        "The class label is assigned by taking the majority label of the $ k $ closest neighbors.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **Random Forest**\n",
        "\n",
        "**Random Forest** is an ensemble learning method that builds multiple decision trees and merges their results for better accuracy and robustness.\n",
        "\n",
        "- **Bagging**: Each tree is trained on a random subset of the data (both features and samples).\n",
        "- **Random Features**: A random subset of features is considered when splitting each node, reducing correlation among trees.\n",
        "\n",
        "#### Mathematical Concept:\n",
        "Random Forest averages the predictions of individual trees in regression or takes a majority vote in classification.\n",
        "\n",
        "For a dataset $ D $ with samples $ (X_1, y_1), (X_2, y_2), \\dots, X_n $:\n",
        "\n",
        "- Multiple decision trees $ T_1, T_2, \\dots, T_m $ are trained on random bootstrapped subsets.\n",
        "- The final prediction for a classification task is:\n",
        "\n",
        "$$\n",
        "\\hat{y} = \\text{mode}(T_1(X), T_2(X), \\dots, T_m(X))\n",
        "$$\n",
        "\n",
        "Where `mode` refers to the most frequent class predicted by the trees.\n",
        "\n",
        "---\n",
        "\n",
        "### 4. **Gradient Boosting**\n",
        "\n",
        "**Gradient Boosting** is an ensemble method where models (usually decision trees) are built sequentially, and each new model focuses on correcting the errors made by the previous ones.\n",
        "\n",
        "- **Boosting**: It works by minimizing a loss function by adding models that correct the errors made by the ensemble.\n",
        "- **Learning Rate**: A parameter that controls the contribution of each tree to the ensemble.\n",
        "\n",
        "---\n",
        "\n",
        "## **Understanding Classification Metrics**\n",
        "\n",
        "### 1. **Accuracy**\n",
        "\n",
        "**Accuracy** measures the percentage of correct predictions out of all predictions.\n",
        "\n",
        "$$\n",
        "\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}\n",
        "$$\n",
        "\n",
        "Where:\n",
        "- **TP** = True Positives (correctly predicted positive cases),\n",
        "- **TN** = True Negatives (correctly predicted negative cases),\n",
        "- **FP** = False Positives (incorrectly predicted positive cases),\n",
        "- **FN** = False Negatives (incorrectly predicted negative cases).\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **Precision**\n",
        "\n",
        "**Precision** measures the proportion of true positive predictions out of all positive predictions (i.e., how many selected items are relevant).\n",
        "\n",
        "$$\n",
        "\\text{Precision} = \\frac{TP}{TP + FP}\n",
        "$$\n",
        "\n",
        "It is useful when the cost of false positives is high (e.g., in spam detection).\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **Recall (Sensitivity or True Positive Rate)**\n",
        "\n",
        "**Recall** measures the proportion of actual positive cases that were correctly identified (i.e., how many relevant items are selected).\n",
        "\n",
        "$$\n",
        "\\text{Recall} = \\frac{TP}{TP + FN}\n",
        "$$\n",
        "\n",
        "It is important when the cost of false negatives is high (e.g., in medical diagnosis).\n",
        "\n",
        "---\n",
        "\n",
        "### 4. **F1-Score**\n",
        "\n",
        "**F1-Score** is the harmonic mean of precision and recall, providing a single metric that balances both.\n",
        "\n",
        "$$\n",
        "F1 = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
        "$$\n",
        "\n",
        "F1-Score is useful when you need a balance between precision and recall.\n",
        "\n",
        "---\n",
        "\n",
        "### 5. **Confusion Matrix**\n",
        "\n",
        "A **Confusion Matrix** is a summary of prediction results. It provides insights into the number of true positives, false positives, true negatives, and false negatives.\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "TP & FP \\\\\n",
        "FN & TN\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "This matrix helps in calculating all other metrics like accuracy, precision, recall, and F1-score.\n",
        "\n",
        "---\n",
        "\n",
        "### 6. **ROC-AUC Score**\n",
        "\n",
        "**ROC (Receiver Operating Characteristic)** curve is a graphical plot that illustrates the diagnostic ability of a binary classifier system as its discrimination threshold is varied.\n",
        "\n",
        "- **AUC (Area Under Curve)** is the area under the ROC curve and provides a single value to compare different models.\n",
        "\n",
        "The **True Positive Rate (TPR)** and **False Positive Rate (FPR)** are used to plot the ROC curve:\n",
        "\n",
        "$$\n",
        "\\text{TPR} = \\frac{TP}{TP + FN}, \\quad \\text{FPR} = \\frac{FP}{FP + TN}\n",
        "$$\n",
        "\n",
        "A perfect model has an AUC of 1, while a random classifier has an AUC of 0.5.\n",
        "\n",
        "---\n",
        "\n",
        "In multiclass classification, many of the metrics used for binary classification can be extended to handle multiple classes. Below is a detailed explanation of **accuracy**, **precision**, **recall**, **F1-score**, and other metrics for multiclass classification.\n",
        "\n",
        "---\n",
        "\n",
        "### **Multiclass Classification Metrics**\n",
        "\n",
        "In multiclass classification, we deal with more than two classes, so metrics like **precision**, **recall**, and **F1-score** are typically computed using one of the following strategies:\n",
        "\n",
        "- **Micro-average**: Aggregates the contributions of all classes to compute the average metric.\n",
        "- **Macro-average**: Computes the metric independently for each class and then takes the average.\n",
        "- **Weighted-average**: Averages the metrics for each class, weighted by the number of instances in each class.\n",
        "\n",
        "Let’s dive into the individual metrics and their formulas for multiclass classification:\n",
        "\n",
        "---\n",
        "\n",
        "### 1. **Accuracy (Multiclass)**\n",
        "\n",
        "Accuracy in multiclass classification is the same as in binary classification, representing the percentage of correct predictions out of all predictions:\n",
        "\n",
        "$$\n",
        "\\text{Accuracy} = \\frac{\\text{Correct Predictions}}{\\text{Total Predictions}} = \\frac{TP_1 + TP_2 + \\dots + TP_k}{n}\n",
        "$$\n",
        "\n",
        "Where:\n",
        "- $TP_i$ represents the true positives for class $i$.\n",
        "- $k$ is the number of classes.\n",
        "- $n$ is the total number of instances.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **Precision (Multiclass)**\n",
        "\n",
        "Precision in multiclass classification can be calculated using either **macro-averaging** or **micro-averaging**:\n",
        "\n",
        "#### **Macro-average Precision**:\n",
        "The precision is calculated for each class separately and then averaged:\n",
        "\n",
        "$$\n",
        "\\text{Precision}_{\\text{macro}} = \\frac{1}{k} \\sum_{i=1}^{k} \\frac{TP_i}{TP_i + FP_i}\n",
        "$$\n",
        "\n",
        "Where:\n",
        "- $TP_i$ = True Positives for class $i$.\n",
        "- $FP_i$ = False Positives for class $i$.\n",
        "- $k$ is the number of classes.\n",
        "\n",
        "#### **Micro-average Precision**:\n",
        "Treats all instances as a flat list of binary classification tasks, then computes precision globally:\n",
        "\n",
        "$$\n",
        "\\text{Precision}_{\\text{micro}} = \\frac{\\sum_{i=1}^{k} TP_i}{\\sum_{i=1}^{k} (TP_i + FP_i)}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **Recall (Multiclass)**\n",
        "\n",
        "Recall, or sensitivity, can be calculated similarly with **macro-averaging** or **micro-averaging**:\n",
        "\n",
        "#### **Macro-average Recall**:\n",
        "The recall is calculated for each class individually and then averaged:\n",
        "\n",
        "$$\n",
        "\\text{Recall}_{\\text{macro}} = \\frac{1}{k} \\sum_{i=1}^{k} \\frac{TP_i}{TP_i + FN_i}\n",
        "$$\n",
        "\n",
        "Where:\n",
        "- $TP_i$ = True Positives for class $i$.\n",
        "- $FN_i$ = False Negatives for class $i$.\n",
        "\n",
        "#### **Micro-average Recall**:\n",
        "Recall is computed globally by summing over all classes:\n",
        "\n",
        "$$\n",
        "\\text{Recall}_{\\text{micro}} = \\frac{\\sum_{i=1}^{k} TP_i}{\\sum_{i=1}^{k} (TP_i + FN_i)}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### 4. **F1-Score (Multiclass)**\n",
        "\n",
        "The **F1-Score** is the harmonic mean of precision and recall. For multiclass classification, it can be computed using **macro-averaging** or **micro-averaging**.\n",
        "\n",
        "#### **Macro-average F1-Score**:\n",
        "The F1-Score is calculated for each class and averaged:\n",
        "\n",
        "$$\n",
        "\\text{F1}_{\\text{macro}} = \\frac{1}{k} \\sum_{i=1}^{k} 2 \\times \\frac{\\text{Precision}_i \\times \\text{Recall}_i}{\\text{Precision}_i + \\text{Recall}_i}\n",
        "$$\n",
        "\n",
        "#### **Micro-average F1-Score**:\n",
        "This approach aggregates the contributions of all classes before calculating the F1-Score:\n",
        "\n",
        "$$\n",
        "\\text{F1}_{\\text{micro}} = 2 \\times \\frac{\\text{Precision}_{\\text{micro}} \\times \\text{Recall}_{\\text{micro}}}{\\text{Precision}_{\\text{micro}} + \\text{Recall}_{\\text{micro}}}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### 5. **Confusion Matrix (Multiclass)**\n",
        "\n",
        "For multiclass classification, the confusion matrix is an extension of the binary confusion matrix. It is an $k \\times k$ matrix, where $k$ is the number of classes.\n",
        "\n",
        "Each element $C_{ij}$ in the confusion matrix represents the number of instances where the true label is class $i$ and the predicted label is class $j$.\n",
        "\n",
        "The confusion matrix for a multiclass problem looks like this:\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "C_{11} & C_{12} & \\dots & C_{1k} \\\\\n",
        "C_{21} & C_{22} & \\dots & C_{2k} \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "C_{k1} & C_{k2} & \\dots & C_{kk} \\\\\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "- $C_{ii}$ = True Positives for class $i$.\n",
        "- $C_{ij}$ (where $i \\neq j$) = Misclassifications.\n",
        "\n",
        "From the confusion matrix, you can compute precision, recall, and F1-scores for each class and then average them.\n",
        "\n",
        "---\n",
        "\n",
        "### 6. **ROC-AUC for Multiclass**\n",
        "\n",
        "In the multiclass setting, **ROC-AUC** can be computed by considering each class against all other classes (one-vs-rest approach). For each class $i$, we compute the ROC curve, treating class $i$ as the positive class and all others as the negative class.\n",
        "\n",
        "The overall **multiclass AUC** can then be calculated by averaging the AUC scores for all classes.\n",
        "\n",
        "For each class $i$, the **True Positive Rate (TPR)** and **False Positive Rate (FPR)** are defined as:\n",
        "\n",
        "$$\n",
        "\\text{TPR}_i = \\frac{TP_i}{TP_i + FN_i}, \\quad \\text{FPR}_i = \\frac{FP_i}{FP_i + TN_i}\n",
        "$$\n",
        "\n",
        "The **ROC-AUC** is then computed as the area under the ROC curve for each class, and the average AUC can be taken across all classes.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZhJUBoh2EFv"
      },
      "source": [
        "#### Example: Training a SVM Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ivjWZp1jzBMV",
        "outputId": "aab7302c-df9d-4070-c900-c53a79376989"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC  # Support Vector Classifier\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# Step 1: Load the digits dataset\n",
        "digits = load_digits()\n",
        "\n",
        "# Step 2: Split the data into 70% train and 30% test\n",
        "X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 3: Initialize the SVM classifier\n",
        "# Here we use the 'rbf' kernel (Radial Basis Function) by default, but you can change it if you want\n",
        "clf = SVC(kernel='rbf')\n",
        "\n",
        "# Step 4: Train the classifier on the training data\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Make predictions on the test data\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Step 6: Evaluate the model\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_voXPfS2Qa3"
      },
      "source": [
        "#### Example: Training a KNN Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zLdePGI808Ht",
        "outputId": "f3c74579-685d-4494-a7f1-5459635009a4"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# Step 1: Load the digits dataset\n",
        "digits = load_digits()\n",
        "\n",
        "# Step 2: Split the data into 70% train and 30% test\n",
        "X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 3: Initialize the KNN classifier\n",
        "# n_neighbors is the number of neighbors to use, we'll use 5 (the default)\n",
        "clf = KNeighborsClassifier(n_neighbors=5)\n",
        "\n",
        "# Step 4: Train the classifier on the training data\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Make predictions on the test data\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Step 6: Evaluate the model\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZtPTqwxI2VSX"
      },
      "source": [
        "#### Example: Training a Random Forest Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SxwjtBu01M0s",
        "outputId": "57800b6d-cbc6-483e-c2e6-0acb28937b55"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# Step 1: Load the digits dataset\n",
        "digits = load_digits()\n",
        "\n",
        "# Step 2: Split the data into 70% train and 30% test\n",
        "X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 3: Initialize the Random Forest classifier\n",
        "# n_estimators is the number of trees in the forest, we'll use 100 (default)\n",
        "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Step 4: Train the classifier on the training data\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Make predictions on the test data\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Step 6: Evaluate the model\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2hjdzQ-2YUQ"
      },
      "source": [
        "#### Example: Training a Gradient Boosting Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0IBCXSR_1lYA",
        "outputId": "4b121621-bc81-4370-a55a-b0a644d1c44e"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# Step 1: Load the digits dataset\n",
        "digits = load_digits()\n",
        "\n",
        "# Step 2: Split the data into 70% train and 30% test\n",
        "X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 3: Initialize the Gradient Boosting classifier\n",
        "# We'll use default parameters for GradientBoostingClassifier\n",
        "clf = GradientBoostingClassifier(random_state=42)\n",
        "\n",
        "# Step 4: Train the classifier on the training data\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Make predictions on the test data\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Step 6: Evaluate the model\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3pyC8WW2eN_"
      },
      "source": [
        "#### Conclusion\n",
        "For this dataset, and with the basic classifier settings, a KNN classifier is the best option (based on the overall accuracy). <br>\n",
        "In the following, we fix the classifier and do further analysis!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "TEERg1tLsHC9",
        "outputId": "73fcab86-f440-4403-fa22-e6ba9ac89193"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, roc_curve, auc\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Load the digits dataset\n",
        "digits = load_digits()\n",
        "\n",
        "# Step 2: Split the data into 70% train and 30% test\n",
        "X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 3: Initialize the KNN classifier (using default neighbors=5)\n",
        "clf = KNeighborsClassifier(n_neighbors=5)\n",
        "\n",
        "# Step 4: Train the classifier\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Make predictions\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Step 6: Evaluate the model\n",
        "classification_report_output = classification_report(y_test, y_pred)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Step 7: Confusion Matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Step 8: Visualizing the Confusion Matrix\n",
        "plt.figure(figsize=(10, 7))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False, xticklabels=digits.target_names, yticklabels=digits.target_names)\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.show()\n",
        "\n",
        "# Step 9: Print Classification Report and Accuracy\n",
        "print(\"Classification Report:\\n\", classification_report_output)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Step 10: ROC-AUC Curve (One-vs-Rest approach for multiclass)\n",
        "y_test_bin = label_binarize(y_test, classes=range(10))  # Binarize for ROC curve\n",
        "y_pred_bin = clf.predict_proba(X_test)\n",
        "\n",
        "fpr = dict()\n",
        "tpr = dict()\n",
        "roc_auc = dict()\n",
        "n_classes = 10\n",
        "\n",
        "# Calculate ROC for each class\n",
        "for i in range(n_classes):\n",
        "    fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_pred_bin[:, i])\n",
        "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "# Plot ROC Curves for the first few classes for visualization\n",
        "plt.figure(figsize=(10, 7))\n",
        "for i in range(3):  # Plot first 3 classes' ROC curves for simplicity\n",
        "    plt.plot(fpr[i], tpr[i], label=f'Class {i} (area = {roc_auc[i]:.2f})')\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve for KNN Classifier (First 3 Classes)')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJKE8B3tmx96"
      },
      "source": [
        "# How Does A Linear SVM Decide?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663
        },
        "id": "Vcaok6ABmW0l",
        "outputId": "94fbfefb-ee21-4769-8375-86cd1fdca584"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# Step 1: Create synthetic dataset with 2 features and 2 classes\n",
        "X, y = make_classification(n_samples=300, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=42)\n",
        "\n",
        "# Step 2: Split the data into 70% train and 30% test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 3: Initialize the SVM classifier with 'rbf' kernel\n",
        "clf = SVC(kernel='linear')\n",
        "\n",
        "# Step 4: Train the classifier on the training data\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Make predictions on the test data\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Step 6: Evaluate the model\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "\n",
        "# Step 7: Plot decision boundaries\n",
        "# Create a meshgrid for plotting\n",
        "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01), np.arange(y_min, y_max, 0.01))\n",
        "\n",
        "# Predict the class for each point in the meshgrid\n",
        "Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "Z = Z.reshape(xx.shape)\n",
        "\n",
        "# Plot the decision boundary\n",
        "plt.contourf(xx, yy, Z, alpha=0.4, cmap=plt.cm.RdYlBu)\n",
        "\n",
        "# Scatter plot of the training data points\n",
        "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, s=30, cmap=plt.cm.RdYlBu, edgecolors='k')\n",
        "plt.title('Linear-Kernel SVM Decision Boundary on Synthetic Data')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsXnkVAfnYAb"
      },
      "source": [
        "# How Does An RBF-Kernel SVM Decide?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663
        },
        "id": "1FwuVRwWndYP",
        "outputId": "a43ae417-8b03-417a-a88c-0fe5d38d62e0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# Step 1: Create synthetic dataset with 2 features and 2 classes\n",
        "X, y = make_classification(n_samples=300, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=42)\n",
        "\n",
        "# Step 2: Split the data into 70% train and 30% test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 3: Initialize the SVM classifier with 'rbf' kernel\n",
        "clf = SVC(kernel='rbf')\n",
        "\n",
        "# Step 4: Train the classifier on the training data\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Make predictions on the test data\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Step 6: Evaluate the model\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "\n",
        "# Step 7: Plot decision boundaries\n",
        "# Create a meshgrid for plotting\n",
        "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01), np.arange(y_min, y_max, 0.01))\n",
        "\n",
        "# Predict the class for each point in the meshgrid\n",
        "Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "Z = Z.reshape(xx.shape)\n",
        "\n",
        "# Plot the decision boundary\n",
        "plt.contourf(xx, yy, Z, alpha=0.4, cmap=plt.cm.RdYlBu)\n",
        "\n",
        "# Scatter plot of the training data points\n",
        "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, s=30, cmap=plt.cm.RdYlBu, edgecolors='k')\n",
        "plt.title('RBF-Kernel SVM Decision Boundary on Synthetic Data')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggxsY_BspNP-"
      },
      "source": [
        "# How Does KNN Decide?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663
        },
        "id": "rdhVxl6mn5jB",
        "outputId": "91b33911-53d2-498b-92f8-49742e4e0c97"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# Step 1: Create synthetic dataset with 2 features and 2 classes\n",
        "X, y = make_classification(n_samples=300, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=42)\n",
        "\n",
        "# Step 2: Split the data into 70% train and 30% test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 3: Initialize the KNN classifier\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "\n",
        "# Step 4: Train the classifier on the training data\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Make predictions on the test data\n",
        "y_pred = knn.predict(X_test)\n",
        "\n",
        "# Step 6: Evaluate the model\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "\n",
        "# Step 7: Plot decision boundaries\n",
        "# Create a meshgrid for plotting\n",
        "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01), np.arange(y_min, y_max, 0.01))\n",
        "\n",
        "# Predict the class for each point in the meshgrid\n",
        "Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "Z = Z.reshape(xx.shape)\n",
        "\n",
        "# Plot the decision boundary\n",
        "plt.contourf(xx, yy, Z, alpha=0.4, cmap=plt.cm.RdYlBu)\n",
        "\n",
        "# Scatter plot of the training data points\n",
        "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, s=30, cmap=plt.cm.RdYlBu, edgecolors='k')\n",
        "plt.title('KNN Decision Boundary on Synthetic Data')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tExVBHQapSVT"
      },
      "source": [
        "# How Does Random Forest Decide?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663
        },
        "id": "IBmGXZncoa6G",
        "outputId": "e520b152-4664-4712-f9c0-f6d6d4ee583a"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# Step 1: Create synthetic dataset with 2 features and 2 classes\n",
        "X, y = make_classification(n_samples=300, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=42)\n",
        "\n",
        "# Step 2: Split the data into 70% train and 30% test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 3: Initialize the Random Forest classifier\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Step 4: Train the classifier on the training data\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Make predictions on the test data\n",
        "y_pred = rf_classifier.predict(X_test)\n",
        "\n",
        "# Step 6: Evaluate the model\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "\n",
        "# Step 7: Plot decision boundaries\n",
        "# Create a meshgrid for plotting\n",
        "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01), np.arange(y_min, y_max, 0.01))\n",
        "\n",
        "# Predict the class for each point in the meshgrid\n",
        "Z = rf_classifier.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "Z = Z.reshape(xx.shape)\n",
        "\n",
        "# Plot the decision boundary\n",
        "plt.contourf(xx, yy, Z, alpha=0.4, cmap=plt.cm.RdYlBu)\n",
        "\n",
        "# Scatter plot of the training data points\n",
        "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, s=30, cmap=plt.cm.RdYlBu, edgecolors='k')\n",
        "plt.title('Random Forest Decision Boundary on Synthetic Data')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZ2LP1ntpXh1"
      },
      "source": [
        "# How Does An Gradient Boosting Decide?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663
        },
        "id": "f8Z0p6jeoqqt",
        "outputId": "b4262efd-9f0b-4844-9424-68db23da4c81"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# Step 1: Create synthetic dataset with 2 features and 2 classes\n",
        "X, y = make_classification(n_samples=300, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=42)\n",
        "\n",
        "# Step 2: Split the data into 70% train and 30% test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 3: Initialize the Gradient Boosting classifier\n",
        "gb_classifier = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Step 4: Train the classifier on the training data\n",
        "gb_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Make predictions on the test data\n",
        "y_pred = gb_classifier.predict(X_test)\n",
        "\n",
        "# Step 6: Evaluate the model\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "\n",
        "# Step 7: Plot decision boundaries\n",
        "# Create a meshgrid for plotting\n",
        "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01), np.arange(y_min, y_max, 0.01))\n",
        "\n",
        "# Predict the class for each point in the meshgrid\n",
        "Z = gb_classifier.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "Z = Z.reshape(xx.shape)\n",
        "\n",
        "# Plot the decision boundary\n",
        "plt.contourf(xx, yy, Z, alpha=0.4, cmap=plt.cm.RdYlBu)\n",
        "\n",
        "# Scatter plot of the training data points\n",
        "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, s=30, cmap=plt.cm.RdYlBu, edgecolors='k')\n",
        "plt.title('Gradient Boosting Decision Boundary on Synthetic Data')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOxIZG21holE"
      },
      "source": [
        "### **Visualizing Decision Boundaries in Reduced Dimensional Space**\n",
        "\n",
        "When working with high-dimensional data, it's often difficult to directly visualize the patterns or separations between different classes. To address this, we can use **dimensionality reduction techniques** like **Principal Component Analysis (PCA)** to project the data into a lower-dimensional space, typically 2D or 3D, making it easier to explore visually.\n",
        "\n",
        "#### **Key Concepts**:\n",
        "\n",
        "1. **Dimensionality Reduction**:\n",
        "   - **PCA** is a technique that transforms the data into a new coordinate system where the axes (called **principal components**) are aligned with the directions of maximum variance in the data. This allows us to capture most of the important information while discarding less significant details. In this case, we reduce the data to two dimensions for visualization.\n",
        "   \n",
        "2. **Decision Boundaries**:\n",
        "   - **Decision boundaries** represent the regions of space where a model changes its classification from one class to another. These boundaries are influenced by the features of the data and the learning algorithm, and they help to define how well a model can separate different classes.\n",
        "   - In a **2D projection**, decision boundaries are often represented as curves or lines that separate regions of different predicted classes. By plotting these boundaries, we can visualize how well the data is separable in the reduced-dimensional space.\n",
        "\n",
        "3. **Meshgrid for Plotting**:\n",
        "   - To visualize decision boundaries, we generate a **meshgrid** that spans the entire 2D feature space. For each point in this grid, we assign a predicted class label based on the learned structure of the data (after PCA transformation).\n",
        "   - The predicted classes for these grid points allow us to create a **contour plot** that visually represents the decision regions.\n",
        "\n",
        "4. **Scatter Plot of Data**:\n",
        "   - On top of the contour plot, we plot the actual data points in the 2D PCA space. This helps to show where the real data falls relative to the decision boundaries, providing insights into how well the data is separated after dimensionality reduction.\n",
        "\n",
        "By projecting the data into two dimensions and visualizing the decision boundaries, we gain a better understanding of how separable the data is in this reduced space. This kind of visualization is particularly useful in tasks such as classification, where understanding the separability of classes is key to improving model performance.\n",
        "\n",
        "---\n",
        "\n",
        "This markdown explains the core concepts behind decision boundaries, dimensionality reduction, and how we visualize these in a 2D projection without focusing solely on the classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "FgRoM-tRCQ5f",
        "outputId": "ed89a8bd-24c9-4c5e-d6f6-3ea672d2388d"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from matplotlib.colors import ListedColormap\n",
        "\n",
        "# Step 1: Load and split the digits dataset\n",
        "digits = load_digits()\n",
        "X = digits.data\n",
        "y = digits.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 2: Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Step 3: Apply PCA to reduce to 2 dimensions\n",
        "pca = PCA(n_components=2)\n",
        "X_train_pca = pca.fit_transform(X_train)\n",
        "X_test_pca = pca.transform(X_test)\n",
        "\n",
        "# Step 4: Train a classifier\n",
        "clf = KNeighborsClassifier(n_neighbors=5)\n",
        "clf.fit(X_train_pca, y_train)\n",
        "\n",
        "# Step 5: Create a meshgrid to plot decision boundary\n",
        "x_min, x_max = X_train_pca[:, 0].min() - 1, X_train_pca[:, 0].max() + 1\n",
        "y_min, y_max = X_train_pca[:, 1].min() - 1, X_train_pca[:, 1].max() + 1\n",
        "grid_x, grid_y = np.meshgrid(np.arange(x_min, x_max, 0.1), np.arange(y_min, y_max, 0.1))\n",
        "\n",
        "# Step 6: Predict class for each point in the meshgrid\n",
        "Z = clf.predict(np.c_[grid_x.ravel(), grid_y.ravel()])\n",
        "Z = Z.reshape(grid_x.shape)\n",
        "\n",
        "# Step 7: Plot decision boundary and scatter plot of the data\n",
        "plt.contourf(grid_x, grid_y, Z, alpha=0.4, cmap=plt.cm.RdYlBu)\n",
        "\n",
        "# Plot the test data points with legend for each class\n",
        "scatter = plt.scatter(X_train_pca[:, 0], X_train_pca[:, 1], c=y_train, s=30, cmap=plt.cm.RdYlBu, edgecolors='k')\n",
        "\n",
        "# Create a legend using unique class labels\n",
        "# Adding a color legend\n",
        "legend1 = plt.legend(*scatter.legend_elements(), title=\"Classes\", loc=\"upper right\")\n",
        "plt.gca().add_artist(legend1)\n",
        "\n",
        "plt.title(\"Decision Boundary in PCA Space\")\n",
        "plt.xlabel(\"PCA Component 1\")\n",
        "plt.ylabel(\"PCA Component 2\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9UpgJcjhC1c_"
      },
      "source": [
        "Let's break it down!\n",
        "\n",
        "### **Step 1: Loading The Dataset**\n",
        "\n",
        "```python\n",
        "digits = load_digits()\n",
        "X = digits.data\n",
        "y = digits.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "```\n",
        "\n",
        "- **Objective**: Load the digits dataset and split it into training and testing sets.\n",
        "- **Explanation**:\n",
        "  - `load_digits()`: Loads the digits dataset, which contains 8x8 pixel images of handwritten digits (0-9) and their corresponding labels.\n",
        "  - `train_test_split(...)`: The data is split into training and testing sets. `test_size=0.2` means 20% of the data will be used for testing, and 80% for training.\n",
        "\n",
        "---\n",
        "\n",
        "### **Step 2: Standardize the Data**\n",
        "\n",
        "```python\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "```\n",
        "\n",
        "- **Objective**: Normalize the data so that all features have the same scale, which is crucial for PCA and many machine learning algorithms.\n",
        "- **Explanation**:\n",
        "  - `StandardScaler()`: This function standardizes features by removing the mean and scaling to unit variance.\n",
        "  - `scaler.fit_transform(X_train)`: Fits the scaler to the training data and applies the transformation. This ensures that the training data has a mean of 0 and a standard deviation of 1 for each feature.\n",
        "  - `scaler.transform(X_test)`: Applies the same transformation to the test data using the parameters learned from the training set.\n",
        "\n",
        "---\n",
        "\n",
        "### **Step 3: Apply PCA to Reduce to 2 Dimensions**\n",
        "\n",
        "```python\n",
        "pca = PCA(n_components=2)\n",
        "X_train_pca = pca.fit_transform(X_train)\n",
        "X_test_pca = pca.transform(X_test)\n",
        "```\n",
        "\n",
        "- **Objective**: Reduce the dimensionality of the data to 2 components so that it can be visualized in a 2D plot.\n",
        "- **Explanation**:\n",
        "  - `PCA(n_components=2)`: Initializes PCA to reduce the data to 2 principal components.\n",
        "  - `pca.fit_transform(X_train)`: Fits the PCA model to the training data and transforms it into the PCA space.\n",
        "  - `pca.transform(X_test)`: Transforms the test data into the same PCA space, so both sets are projected onto the same 2D plane.\n",
        "\n",
        "---\n",
        "\n",
        "### **Step 4: Train a Classifier**\n",
        "\n",
        "```python\n",
        "clf = KNeighborsClassifier(n_neighbors=5)\n",
        "clf.fit(X_train_pca, y_train)\n",
        "```\n",
        "\n",
        "- **Objective**: Train a K-Nearest Neighbors (KNN) classifier on the PCA-reduced data.\n",
        "- **Explanation**:\n",
        "  - `KNeighborsClassifier(n_neighbors=5)`: Initializes a KNN classifier with 5 neighbors.\n",
        "  - `clf.fit(X_train_pca, y_train)`: Fits the KNN model to the training data (`X_train_pca` is the PCA-transformed training data, and `y_train` is the corresponding label).\n",
        "\n",
        "---\n",
        "\n",
        "### **Step 5: Create a Meshgrid to Plot Decision Boundary**\n",
        "\n",
        "```python\n",
        "x_min, x_max = X_train_pca[:, 0].min() - 1, X_train_pca[:, 0].max() + 1\n",
        "y_min, y_max = X_train_pca[:, 1].min() - 1, X_train_pca[:, 1].max() + 1\n",
        "grid_x, grid_y = np.meshgrid(np.arange(x_min, x_max, 0.1), np.arange(y_min, y_max, 0.1))\n",
        "```\n",
        "\n",
        "- **Objective**: Create a grid of points spanning the PCA-reduced feature space. This grid will be used to visualize the decision boundary.\n",
        "- **Explanation**:\n",
        "  - `X_train_pca[:, 0].min()` and `.max()`: Find the minimum and maximum values of the first principal component.\n",
        "  - `X_train_pca[:, 1].min()` and `.max()`: Find the minimum and maximum values of the second principal component.\n",
        "  - `np.meshgrid(...)`: Creates a 2D grid of points across the PCA space, with step size `0.1` between grid points.\n",
        "\n",
        "---\n",
        "\n",
        "### **Step 6: Predict Class for Each Point in the Meshgrid**\n",
        "\n",
        "```python\n",
        "Z = clf.predict(np.c_[grid_x.ravel(), grid_y.ravel()])\n",
        "Z = Z.reshape(grid_x.shape)\n",
        "```\n",
        "\n",
        "- **Objective**: Use the classifier to predict the class label for each point in the grid, which will be used to plot the decision boundary.\n",
        "- **Explanation**:\n",
        "  - `grid_x.ravel()` and `grid_y.ravel()`: Flatten the meshgrid arrays into 1D arrays.\n",
        "  - `np.c_[...]`: Combines the flattened grid arrays into a 2D array where each row is a (x, y) point.\n",
        "  - `clf.predict(...)`: Predicts the class label for each point in the grid using the trained KNN classifier.\n",
        "  - `Z.reshape(grid_x.shape)`: Reshapes the predicted labels back into the shape of the meshgrid for contour plotting.\n",
        "\n",
        "---\n",
        "\n",
        "### **Step 7: Plot the Decision Boundary and Scatter Plot of the Data**\n",
        "\n",
        "```python\n",
        "plt.contourf(grid_x, grid_y, Z, alpha=0.4, cmap=plt.cm.RdYlBu)\n",
        "scatter = plt.scatter(X_train_pca[:, 0], X_train_pca[:, 1], c=y_train, s=30, cmap=plt.cm.RdYlBu, edgecolors='k')\n",
        "```\n",
        "\n",
        "- **Objective**: Visualize the decision boundary (contour plot) and the actual training data points (scatter plot).\n",
        "- **Explanation**:\n",
        "  - `plt.contourf(grid_x, grid_y, Z, alpha=0.4, cmap=plt.cm.RdYlBu)`: Creates a filled contour plot based on the predicted class labels (`Z`). The different regions of the plot are colored according to the predicted class.\n",
        "    - `alpha=0.4`: Makes the contour plot semi-transparent so the data points are still visible.\n",
        "    - `cmap=plt.cm.RdYlBu`: Specifies the color map for the plot.\n",
        "  - `plt.scatter(X_train_pca[:, 0], X_train_pca[:, 1], c=y_train, s=30, cmap=plt.cm.RdYlBu, edgecolors='k')`: Creates a scatter plot of the training data points, colored based on their true labels (`y_train`).\n",
        "\n",
        "---\n",
        "\n",
        "### **Legend for Class Labels**\n",
        "\n",
        "```python\n",
        "legend1 = plt.legend(*scatter.legend_elements(), title=\"Classes\", loc=\"upper right\")\n",
        "plt.gca().add_artist(legend1)\n",
        "```\n",
        "\n",
        "- **Objective**: Add a legend to the plot that shows the class labels and their corresponding colors.\n",
        "- **Explanation**:\n",
        "  - `scatter.legend_elements()`: Creates legend entries for the unique class labels.\n",
        "  - `plt.legend(...)`: Adds a legend to the plot, positioning it in the upper-right corner.\n",
        "  - `plt.gca().add_artist(legend1)`: Adds the legend to the current axes.\n",
        "\n",
        "---\n",
        "\n",
        "### **Final Result**:\n",
        "\n",
        "- **Decision boundary**: The contour plot shows the regions where the classifier predicts each class in the PCA-reduced space.\n",
        "- **Scatter plot**: The data points are plotted in PCA space and colored by their true class labels, providing a clear visual comparison of the actual data and the decision boundary."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBGnSsm3Wfv1"
      },
      "source": [
        "## Advanced Boosting Libraries\n",
        "\n",
        "Here's the updated markdown, including the installation instructions for each library:\n",
        "\n",
        "---\n",
        "\n",
        "## **Introduction to Advanced Boosting Libraries: XGBoost, CatBoost, and LightGBM**\n",
        "\n",
        "Boosting is a powerful ensemble technique that combines multiple weak learners (often decision trees) to create a strong model. It works by iteratively training models, with each new model focusing on the errors made by the previous ones. Among the most popular and advanced boosting algorithms are **XGBoost**, **CatBoost**, and **LightGBM**. Each of these libraries has distinct features that make them efficient and widely used in machine learning competitions and real-world applications.\n",
        "\n",
        "### 1. **XGBoost (Extreme Gradient Boosting)**\n",
        "\n",
        "**XGBoost** is one of the most popular and mature libraries for gradient boosting and is known for its speed, performance, and flexibility.\n",
        "\n",
        "- **Key Features**:\n",
        "  - **Regularization**: XGBoost introduces L1 (Lasso) and L2 (Ridge) regularization techniques to prevent overfitting, making it robust for larger datasets.\n",
        "  - **Handling missing values**: XGBoost automatically learns how to handle missing values in the data.\n",
        "  - **Tree pruning**: It uses an advanced tree-pruning algorithm called \"max_depth\" to avoid overfitting.\n",
        "  - **Sparsity-aware algorithm**: XGBoost has optimizations for handling sparse data.\n",
        "  - **Parallelism**: XGBoost is optimized for parallel computation, making it very efficient, especially with large datasets.\n",
        "  \n",
        "- **Best Used For**:\n",
        "  - Structured/tabular data\n",
        "  - Time series predictions\n",
        "  - Applications requiring high performance and scalability\n",
        "\n",
        "- **Installation**:\n",
        "    ```bash\n",
        "    !pip install xgboost\n",
        "    ```\n",
        "\n",
        "- **Code Example**:\n",
        "    ```python\n",
        "    from xgboost import XGBClassifier\n",
        "    \n",
        "    model = XGBClassifier()\n",
        "    model.fit(X_train, y_train)\n",
        "    ```\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **CatBoost**\n",
        "\n",
        "**CatBoost** (Categorical Boosting) is a gradient boosting library developed by Yandex. Its main strength lies in its ability to handle categorical features efficiently, without the need for extensive preprocessing like one-hot encoding.\n",
        "\n",
        "- **Key Features**:\n",
        "  - **Handles categorical data natively**: CatBoost automatically deals with categorical features without requiring transformations like label encoding or one-hot encoding.\n",
        "  - **Ordered Boosting**: It uses an innovative technique called \"ordered boosting\" to reduce the bias that can occur in boosting algorithms.\n",
        "  - **Fast and easy to use**: CatBoost often requires less hyperparameter tuning and gives great results out of the box.\n",
        "  - **CPU and GPU support**: It supports both CPU and GPU training for faster computations.\n",
        "  \n",
        "- **Best Used For**:\n",
        "  - Datasets with categorical features\n",
        "  - High-dimensional data with many features\n",
        "  - Scenarios where minimal preprocessing is preferred\n",
        "\n",
        "- **Installation**:\n",
        "    ```bash\n",
        "    !pip install catboost\n",
        "    ```\n",
        "\n",
        "- **Code Example**:\n",
        "    ```python\n",
        "    from catboost import CatBoostClassifier\n",
        "    \n",
        "    model = CatBoostClassifier()\n",
        "    model.fit(X_train, y_train)\n",
        "    ```\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **LightGBM (Light Gradient Boosting Machine)**\n",
        "\n",
        "**LightGBM** is an open-source, highly efficient gradient boosting library developed by Microsoft. It is designed for fast training with large datasets and low memory usage, making it suitable for high-dimensional and large-scale data.\n",
        "\n",
        "- **Key Features**:\n",
        "  - **Leaf-wise tree growth**: Unlike the level-wise growth of traditional gradient boosting, LightGBM uses a leaf-wise growth strategy, which makes it more efficient and reduces overfitting.\n",
        "  - **Histogram-based algorithm**: LightGBM uses histogram-based algorithms that allow for faster training and reduced memory usage.\n",
        "  - **Supports large datasets**: LightGBM is optimized for performance with large datasets and high-dimensional data.\n",
        "  - **Efficient GPU training**: LightGBM can also leverage GPU for accelerated training.\n",
        "  \n",
        "- **Best Used For**:\n",
        "  - Large-scale datasets\n",
        "  - High-dimensional data\n",
        "  - Real-time predictions requiring fast inference\n",
        "\n",
        "- **Installation**:\n",
        "    ```bash\n",
        "    !pip install lightgbm\n",
        "    ```\n",
        "\n",
        "- **Code Example**:\n",
        "    ```python\n",
        "    import lightgbm as lgb\n",
        "    \n",
        "    model = lgb.LGBMClassifier()\n",
        "    model.fit(X_train, y_train)\n",
        "    ```\n",
        "\n",
        "---\n",
        "\n",
        "### **Comparison Overview**\n",
        "\n",
        "| Feature                 | **XGBoost**             | **CatBoost**           | **LightGBM**           |\n",
        "|-------------------------|-------------------------|------------------------|------------------------|\n",
        "| **Strength**             | Regularization, flexibility, parallelization | Handles categorical data natively | Speed and efficiency for large datasets |\n",
        "| **Handling Categorical Features** | Requires encoding        | Native handling         | Requires encoding       |\n",
        "| **Training Speed**       | Fast with parallelization | Fast and requires little tuning | Extremely fast with large data |\n",
        "| **GPU Support**          | Yes                     | Yes                    | Yes                    |\n",
        "| **Best For**             | Structured/tabular data, large datasets | Categorical-heavy datasets | Large datasets, real-time predictions |\n",
        "\n",
        "---\n",
        "\n",
        "### **Conclusion**\n",
        "\n",
        "Each of these libraries offers powerful, efficient implementations of gradient boosting, with strengths tailored to specific use cases.\n",
        "\n",
        "- **XGBoost** shines in flexibility, speed, and regularization, making it an industry favorite for high-performance applications.\n",
        "- **CatBoost** stands out for its ability to handle categorical data naturally, making it a go-to choice when dealing with datasets with many categorical variables.\n",
        "- **LightGBM** excels at handling large datasets with superior speed and low memory usage, especially suited for high-dimensional and large-scale data.\n",
        "\n",
        "By understanding their unique advantages, you can choose the right boosting library based on your dataset and requirements."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z023gkf4aCq2"
      },
      "source": [
        "To display Python code properly in Markdown, you can use triple backticks (` ``` `) to indicate code blocks. Here’s a revised markdown that correctly formats the Python code:\n",
        "\n",
        "---\n",
        "\n",
        "# A Small Exercise!\n",
        "\n",
        "- Create your synthetic classification data.\n",
        "- Observe the data separability in different embeddings (PCA, kernel PCA, t-SNE, UMAP, etc.).\n",
        "\n",
        "\n",
        "**Hints**:\n",
        "\n",
        "```\n",
        "pip install umap-learn\n",
        "```\n",
        "\n",
        "\n",
        "```python\n",
        "from sklearn.decomposition import PCA, KernelPCA\n",
        "from sklearn.manifold import TSNE\n",
        "import umap\n",
        "```\n",
        "\n",
        "```python\n",
        "# PCA (Linear)\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X)\n",
        "\n",
        "# Kernel PCA (Non-linear) with RBF kernel\n",
        "kpca = KernelPCA(n_components=2, kernel='rbf')\n",
        "X_kpca = kpca.fit_transform(X)\n",
        "\n",
        "# t-SNE (Non-linear)\n",
        "tsne = TSNE(n_components=2, random_state=42)\n",
        "X_tsne = tsne.fit_transform(X)\n",
        "\n",
        "# UMAP (Non-linear)\n",
        "umap_model = umap.UMAP(n_components=2, random_state=42)\n",
        "X_umap = umap_model.fit_transform(X)\n",
        "```\n",
        "\n",
        "```python\n",
        "# Define a helper function for plotting\n",
        "def plot_embedding(X_embedded, y, title):\n",
        "    plt.figure(figsize=(6, 6))\n",
        "    plt.scatter(X_embedded[:, 0], X_embedded[:, 1], c=y, cmap='Spectral', s=10)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    plt.show()\n",
        "\n",
        "# Plot the results\n",
        "plot_embedding(X_pca, y, 'PCA (Linear)')\n",
        "plot_embedding(X_kpca, y, 'Kernel PCA (RBF Kernel)')\n",
        "plot_embedding(X_tsne, y, 't-SNE')\n",
        "plot_embedding(X_umap, y, 'UMAP')\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
