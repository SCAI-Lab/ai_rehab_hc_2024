{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCcGBmatBqCU"
      },
      "source": [
        "Some important Python libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UiwXEO_JAB8P",
        "outputId": "8c161f44-528b-4d02-8029-aacfd53493c4"
      },
      "outputs": [],
      "source": [
        "!pip install numpy pandas scikit-learn matplotlib seaborn scipy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GU-3NewLNJF"
      },
      "source": [
        "### Data Visualization\n",
        "OS → Excel <br>\n",
        "Python → Pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "GOfPbwhJAxNV",
        "outputId": "c484dc78-d5ba-4e93-86d1-4902c58d0d21"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Creating a DataFrame with house data\n",
        "data = {\n",
        "    'Bedrooms': [2, 3, 4, 3, 5],         # Feature 1\n",
        "    'Bathrooms': [1, 2, 2, 1, 3],        # Feature 2\n",
        "    'SquareFeet': [1500, 2000, 2500, 1800, 3000],  # Feature 3\n",
        "    'Price': [200000, 250000, 300000, 220000, 350000]  # Target variable\n",
        "}\n",
        "\n",
        "# Create the DataFrame\n",
        "house_data = pd.DataFrame(data)\n",
        "\n",
        "# Display the DataFrame\n",
        "display(house_data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VaHk1KuUJ7bN"
      },
      "source": [
        "Loading Diabetes dataset (normalized) from sklearn\n",
        "\n",
        "age: Age in years <br>\n",
        "sex: Gender of the patient <br>\n",
        "bmi: Body mass index <br>\n",
        "bp: Average blood pressure <br>\n",
        "s1: Total serum cholesterol (tc) <br>\n",
        "s2: Low-density lipoproteins (ldl) <br>\n",
        "s3: High-density lipoproteins (hdl) <br>\n",
        "s4: Total cholesterol / HDL (tch) <br>\n",
        "s5: Possibly log of serum triglycerides level (ltg) <br>\n",
        "s6: Blood sugar level (glu) <br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "lml04MefI0IH",
        "outputId": "33c6019e-1c1e-49e3-e507-703409bdbd46"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_diabetes\n",
        "import pandas as pd\n",
        "\n",
        "# Load the diabetes dataset\n",
        "diabetes = load_diabetes()\n",
        "\n",
        "# Create a DataFrame from the dataset\n",
        "diabetes_data = pd.DataFrame(diabetes.data, columns=diabetes.feature_names)\n",
        "\n",
        "# Add the target variable (disease progression)\n",
        "diabetes_data['DiseaseProgression'] = diabetes.target\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "diabetes_data.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-XJ3oVwaaaMB",
        "outputId": "dc39835b-6240-4b74-8aa5-2ad4fb0327df"
      },
      "outputs": [],
      "source": [
        "diabetes_data.info()  # General information about columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "id": "YROwiyvNapqM",
        "outputId": "4050ed09-213f-42fa-955d-38b8ac7719d7"
      },
      "outputs": [],
      "source": [
        "diabetes_data.describe()  # Summary statistics for numerical data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OqudwuueGOWa"
      },
      "source": [
        "Simple visualization of your dataframe\n",
        "* Bar Plot: Displays how house prices vary with the number of bedrooms. Each bar represents a house, and the height indicates the price.\n",
        "* Scatter Plot: Shows if there is a positive or negative relationship between square footage and price. We expect larger houses to generally have higher prices.\n",
        "* Histogram: Shows how house prices are distributed across the dataset.\n",
        "* Correlation Heatmap: This shows the relationship between all features. A correlation closer to 1 means a strong positive relationship (e.g., SquareFeet and Price might be highly correlated)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "9tjzx7vvJDg9",
        "outputId": "c2ecc11b-ddda-401d-dea4-345643cfe01f"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Scatter plot for BMI vs Disease Progression\n",
        "diabetes_data.plot(kind='scatter', x='bmi', y='DiseaseProgression', title='BMI vs Disease Progression', color='blue')\n",
        "\n",
        "plt.ylabel('Disease Progression')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "Dz_4eOrBJTox",
        "outputId": "f899c50f-f017-4506-dee7-cb5dd1d003ba"
      },
      "outputs": [],
      "source": [
        "# Histogram of Disease Progression\n",
        "diabetes_data['DiseaseProgression'].plot(kind='hist', bins=30, title='Distribution of Disease Progression', color='green')\n",
        "\n",
        "plt.xlabel('Disease Progression')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 821
        },
        "id": "8Z0DH6pkKZTc",
        "outputId": "131f2b89-2aaa-4b0a-abaa-2d7d442c9b4c"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Calculate the correlation matrix\n",
        "corr_matrix = diabetes_data.corr()\n",
        "\n",
        "# Set the figure size (e.g., 12x8 inches)\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "# Visualize the correlation matrix using a heatmap\n",
        "sns.heatmap(corr_matrix, annot=True, cmap=\"coolwarm\")\n",
        "\n",
        "plt.title(\"Feature Correlation Heatmap\", fontsize=16)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "OmRkdg6xKoTw",
        "outputId": "e2d7e112-2053-4616-dcfc-253f6cf75014"
      },
      "outputs": [],
      "source": [
        "# Distribution plot for Age\n",
        "sns.histplot(diabetes_data['age'], kde=True, color='purple')\n",
        "\n",
        "plt.title('Distribution of Age in the Dataset')\n",
        "plt.xlabel('Age')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDEAR6XSPSmN"
      },
      "source": [
        "Here are the key reasons why **normalization of features** is important in machine learning:\n",
        "\n",
        "### 1. **Improving Model Performance**\n",
        "   - **Prevents Dominance by Features with Larger Scales**: In datasets where features have different ranges (e.g., one feature could range from 0 to 1, while another could range from 0 to 10,000), models like linear regression, k-nearest neighbors (k-NN), and support vector machines (SVM) might weigh the larger feature more heavily, even if it’s not more important.\n",
        "   - **Ensures Equal Contribution**: Normalizing scales down all features to a similar range, making sure each feature contributes equally to the learning process.\n",
        "\n",
        "### 2. **Faster Convergence in Gradient Descent**\n",
        "   - **Helps Optimization Algorithms**: For models that use gradient descent (like logistic regression, neural networks, etc.), normalization speeds up convergence by allowing the optimization algorithm to take more even steps in all directions. Features with large values can slow down or complicate the optimization process.\n",
        "   - **Avoids Slow Learning in Some Directions**: If one feature has a much larger scale than others, gradient descent will oscillate inefficiently along the larger feature axis, slowing down learning.\n",
        "\n",
        "### 3. **Improving Accuracy in Distance-Based Algorithms**\n",
        "   - **Important for Distance Metrics**: Algorithms like k-NN, SVMs, and clustering techniques (e.g., K-means) rely on distance calculations between data points. Features with larger ranges can dominate the distance metric, skewing the results.\n",
        "   - **Creates Balanced Influence**: Normalizing ensures that all features have equal influence on the distance computations, leading to better and more accurate results.\n",
        "\n",
        "### 4. **Required for Regularization**\n",
        "   - **Avoids Feature-Scale Bias in Regularization**: When using models that apply regularization (e.g., Ridge or Lasso regression), normalization is crucial. Without normalization, regularization would penalize large coefficients more heavily simply because of the feature scale, leading to suboptimal models.\n",
        "   \n",
        "### 5. **Better Interpretation of Coefficients**\n",
        "   - **Makes Coefficients Comparable**: In linear models, the learned coefficients represent the importance of each feature. Without normalization, interpreting these coefficients becomes tricky, as they are affected by the feature's scale. Normalization puts all features on the same scale, so the magnitude of the coefficients better reflects the feature importance.\n",
        "\n",
        "### 6. **Prepares Data for Neural Networks**\n",
        "   - **Necessary for Activation Functions**: Many neural networks use activation functions like sigmoid, ReLU, or tanh, which perform best when input values are within a small, standardized range (typically between -1 and 1). Normalization ensures that the inputs to the network are well-scaled for these activations, improving training performance and stability.\n",
        "\n",
        "### 7. **Reduces Computational Complexity**\n",
        "   - **Avoids Large Number Handling**: If some features have extremely large values, it may increase the computational complexity or even lead to overflow issues during model training. Normalization avoids such problems by keeping the values within manageable ranges.\n",
        "\n",
        "### 8. **Required for PCA and Other Dimensionality Reduction Methods**\n",
        "   - **Improves Variance Interpretation**: Principal Component Analysis (PCA) and other dimensionality reduction techniques aim to capture the variance in data. Without normalization, features with larger scales would dominate the variance, leading to poor results. Normalization ensures that PCA treats all features equally and captures meaningful variance.\n",
        "\n",
        "### 9. **Improves Training Stability**\n",
        "   - **Avoids Instability Due to Feature Imbalance**: In some models, feature imbalance (due to differences in scale) can cause instability in model training, leading to divergent results or poor performance. Normalization stabilizes the learning process by aligning all feature scales.\n",
        "\n",
        "### Conclusion:\n",
        "Normalization helps balance feature contributions, speeds up model convergence, and ensures better performance for distance-based models and optimization algorithms. It is a crucial preprocessing step for many machine learning models to function efficiently and accurately."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFAiCnhkNd3W"
      },
      "source": [
        "### Imputation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "u73UOWsNd5ed",
        "outputId": "71aa0941-4db8-45e6-9cb4-b8fb889504e5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Creating a DataFrame with missing values\n",
        "data = {\n",
        "    'Age': [25, np.nan, 35, 28, np.nan, 40],\n",
        "    'Salary': [50000, 60000, np.nan, 52000, 48000, np.nan],\n",
        "    'City': ['New York', 'San Francisco', 'Los Angeles', np.nan, 'Chicago', 'Miami']\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Display the DataFrame with missing values\n",
        "display(df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h4fBG8_ueI12",
        "outputId": "e51942d3-6078-419f-e966-7bf9cdc1cf6e"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "agkZq4Xyecaa",
        "outputId": "19893c1e-9c3f-4064-e110-e43eed71596e"
      },
      "outputs": [],
      "source": [
        "# Dropping rows with missing values\n",
        "df_dropped = df.dropna()\n",
        "\n",
        "# Display the DataFrame after dropping missing values\n",
        "display(df_dropped)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "4O6DZb_Nej1n",
        "outputId": "ddc8005c-4393-4590-e3de-f38aecfaea9c"
      },
      "outputs": [],
      "source": [
        "# Filling missing values in 'Age' column with the mean\n",
        "df['Age'] = df['Age'].fillna(df['Age'].mean())\n",
        "\n",
        "# Display the DataFrame after filling missing Age values\n",
        "display(df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "tolRxsy1eqha",
        "outputId": "f5fde452-439c-47a0-f536-f2031536942c"
      },
      "outputs": [],
      "source": [
        "# Filling missing values in 'Salary' column with the median\n",
        "df['Salary'] = df['Salary'].fillna(df['Salary'].median())\n",
        "\n",
        "# Display the DataFrame after filling missing Salary values\n",
        "display(df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "NWXP4NK7euaA",
        "outputId": "5f6e1e78-ea5e-4f47-a17e-abd1687ee3bb"
      },
      "outputs": [],
      "source": [
        "# Filling missing values in 'City' column with the mode (most frequent value)\n",
        "df['City'] = df['City'].fillna(df['City'].mode()[0])\n",
        "\n",
        "# Display the DataFrame after filling missing City values\n",
        "display(df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXjNuRcvfLQW"
      },
      "source": [
        "### Conclusion on Handling Missing Values\n",
        "\n",
        "1. **Dropping Rows**:\n",
        "   - **When to Use**: Dropping rows is useful when the dataset is large, and only a small proportion of rows have missing values. In these cases, removing rows with missing data won’t significantly impact the quality of the dataset or the model’s performance. However, this method is not suitable when the missing data is significant or contains important information.\n",
        "   - **Caution**: Be careful when using this approach, as it may introduce bias if the missing data has a pattern or is not randomly distributed.\n",
        "\n",
        "2. **Filling with Mean/Median/Mode (Imputation)**:\n",
        "   - **Mean Imputation**: Appropriate for numerical data with no significant skew. This method fills in missing values with the average of the non-missing data points. It’s simple and maintains the distribution of data fairly well when the distribution is close to normal.\n",
        "     - **When to Use**: Use mean imputation when the feature’s data is normally distributed and there are few outliers.\n",
        "   \n",
        "   - **Median Imputation**: More suitable for **skewed data**, as it is less affected by outliers. This method replaces missing values with the median of the existing data. Median imputation is robust and is often preferred for features with highly skewed distributions (e.g., income, housing prices).\n",
        "     - **When to Use**: Use median imputation when the data has outliers or a non-symmetric (skewed) distribution, as it provides a better representation of central tendency in such cases.\n",
        "\n",
        "   - **Mode Imputation**: Common for **categorical features**. The mode, or the most frequent value in a column, is used to fill in missing data. This is particularly useful for categorical data where it makes sense to fill missing values with the most common category (e.g., filling missing values in a column with \"Male\" or \"Female\" based on the majority).\n",
        "     - **When to Use**: Use mode imputation for categorical variables, where the most frequent value is a reasonable assumption for filling in missing data.\n",
        "\n",
        "3. **Advanced Imputation (Scikit-learn's SimpleImputer)**:\n",
        "   - **What it Does**: `SimpleImputer` from `scikit-learn` provides more flexibility and control over how missing data is handled. It can impute missing values using strategies like mean, median, mode (most frequent), or even a constant value.\n",
        "   - **When to Use**: This method is useful for integrating imputation into a machine learning pipeline, where missing data is automatically handled during model training and testing.\n",
        "     - **Benefits**: `SimpleImputer` can handle missing data across multiple columns and ensures consistency in imputation when transforming datasets for model training.\n",
        "\n",
        "   - **Alternative Techniques**: More advanced techniques include **K-nearest neighbors imputation** or **regression-based imputation**, where missing values are predicted based on other features in the dataset. These techniques can sometimes offer better accuracy but come with increased complexity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DPsrGr7NzJD"
      },
      "source": [
        "## Time-series"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 753
        },
        "id": "Iyn92sqwhQy0",
        "outputId": "51290ba4-ffae-4517-800f-7c11c16bbb7b"
      },
      "outputs": [],
      "source": [
        "# Here is a Python code to generate an artificial time series signal,\n",
        "# and a time column with the Zurich timezone:\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import pytz\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Generate an artificial time series signal with peaks and dims\n",
        "np.random.seed(42)  # For reproducibility\n",
        "time_points = 1000  # Number of time points\n",
        "time_series = np.linspace(0, 50, time_points)  # Simulating a time axis\n",
        "\n",
        "# Creating a synthetic signal with peaks and dips\n",
        "signal = (\n",
        "    np.sin(time_series) * np.sin(0.5 * time_series) * 10 +\n",
        "    np.cos(2 * time_series) * 2 +\n",
        "    np.random.normal(scale=0.5, size=time_points)\n",
        ")\n",
        "\n",
        "# Create a time column in Zurich timezone (CET/CEST)\n",
        "start_time = datetime(2024, 10, 3, 5, 0, tzinfo=pytz.timezone('Europe/Zurich'))\n",
        "time_column = [start_time + timedelta(minutes=15 * i) for i in range(time_points)]\n",
        "\n",
        "# Create a DataFrame with the time and signal\n",
        "df_time_series = pd.DataFrame({\n",
        "    'Time': time_column,\n",
        "    'Time_series': signal\n",
        "})\n",
        "\n",
        "# Plot the artificial signal\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(df_time_series['Time'], df_time_series['Time_series'], label='Artificial Signal', color='blue')\n",
        "plt.title('An Exemplary Time Series')\n",
        "plt.xlabel('Time (Zurich)')\n",
        "plt.ylabel('Time_series')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Display the first few rows of the DataFrame\n",
        "df_time_series.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "fMZIm9p7hvqk",
        "outputId": "e8ff3366-78f3-4846-a99c-5076328760bb"
      },
      "outputs": [],
      "source": [
        "# Create a new column 'Timestamp' by converting the 'Time' column to UNIX timestamps\n",
        "df_time_series['Timestamp'] = df_time_series['Time'].apply(lambda x: x.timestamp())\n",
        "\n",
        "# Display the first few rows of the DataFrame with the new 'Timestamp' column\n",
        "df_time_series[['Timestamp', 'Time_series']].head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "zEoebTIv_yAE",
        "outputId": "a7ee1180-c2c7-454c-b3e5-8faf9c8ec26d"
      },
      "outputs": [],
      "source": [
        "# Define the start and end datetimes\n",
        "start_datetime = datetime(2024, 10, 3, 6, 0, tzinfo=pytz.timezone('Europe/Zurich'))  # Example start datetime\n",
        "end_datetime = datetime(2024, 10, 3, 7, 0, tzinfo=pytz.timezone('Europe/Zurich'))    # Example end datetime\n",
        "\n",
        "# Convert the datetimes to UNIX timestamps\n",
        "start_timestamp = start_datetime.timestamp()\n",
        "end_timestamp = end_datetime.timestamp()\n",
        "\n",
        "# Select the chunk of the DataFrame between the two timestamps\n",
        "signal_chunk = df_time_series[(df_time_series['Timestamp'] >= start_timestamp) &\n",
        "                              (df_time_series['Timestamp'] <= end_timestamp)]\n",
        "\n",
        "# Display the filtered DataFrame\n",
        "signal_chunk[['Time', 'Time_series', 'Timestamp']]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "htcnkV9eC7b2"
      },
      "source": [
        "# Sampling\n",
        "The Nyquist–Shannon sampling theorem is a fundamental principle in the field of signal processing and information theory. It states that for a continuous-time signal to be properly reconstructed from its sampled values, the sampling rate must be bigger than at least twice the highest frequency component present in the signal.\n",
        "\n",
        "Mathematically, the theorem can be expressed as:\n",
        "\n",
        "f<sub>s</sub> > 2 * f<sub>max</sub>\n",
        "\n",
        "Where:\n",
        "\n",
        "- f<sub>s</sub> is the sampling rate (samples per second)\n",
        "- f<sub>max</sub> is the maximum frequency present in the signal\n",
        "- f<sub>s</sub> = 2 * f<sub>max</sub>, is known as the Nyquist rate.\n",
        "\n",
        "If f<sub>s</sub> > 2 * f<sub>max</sub> is not met, a phenomenon known as aliasing occurs. Aliasing is the effect where high-frequency components in the original signal appear as lower frequencies in the sampled signal, leading to distortion and incorrect reconstruction of the original signal.\n",
        "\n",
        "\n",
        "The Nyquist–Shannon sampling theorem has important implications in various fields, such as digital signal processing, audio and video processing, communications, and data acquisition systems. It provides a theoretical foundation for determining the minimum sampling rate required to properly represent and reconstruct continuous-time signals in the digital domain.\n",
        "\n",
        "* Question: Why are 44100Hz and 48000Hz sampling rates quite popular for audio signals?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        },
        "id": "KkrhUqmZDH20",
        "outputId": "536e6792-e3c3-4377-b328-a3cfb73b4f1e"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.animation import FuncAnimation\n",
        "from IPython.display import HTML\n",
        "\n",
        "# Create a signal with two different frequencies\n",
        "fs = 10000  # Original sampling frequency\n",
        "t = np.linspace(0, 1, fs, endpoint=False)  # Time vector\n",
        "f1 = 5  # Frequency of the first sine wave\n",
        "f2 = 50  # Frequency of the second sine wave\n",
        "signal = np.sin(2 * np.pi * f1 * t) + np.sin(2 * np.pi * f2 * t)\n",
        "\n",
        "# Define sampling rates around the Nyquist rate\n",
        "nyquist_rate = 2 * f2  # 2 * highest frequency\n",
        "sampling_rates = [nyquist_rate / 4, nyquist_rate / 2, nyquist_rate, nyquist_rate , 1.5 * nyquist_rate, 2 * nyquist_rate, 3 * nyquist_rate]\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "line, = ax.plot([], [], lw=2, label='Sampled Signal')\n",
        "points, = ax.plot([], [], 'ro', label='Intersection Points')\n",
        "ax.plot(t, signal, 'k--', lw=1, label='Original Signal')\n",
        "ax.set_xlim(0, 1)\n",
        "ax.set_ylim(-2, 2)\n",
        "ax.set_title('Effect of Different Sampling Rates on Signal')\n",
        "ax.set_xlabel('Time [s]')\n",
        "ax.set_ylabel('Amplitude')\n",
        "\n",
        "# Position legend outside the plot area\n",
        "ax.legend(loc='upper left', bbox_to_anchor=(0.9, 1.15), borderaxespad=0.)\n",
        "\n",
        "def init():\n",
        "    line.set_data([], [])\n",
        "    points.set_data([], [])\n",
        "    return line, points\n",
        "\n",
        "def animate(i):\n",
        "    rate = sampling_rates[i]\n",
        "    t_sampled = t[::int(fs/rate)]\n",
        "\n",
        "    signal_sampled = np.sin(2 * np.pi * f1 * t_sampled) + np.sin(2 * np.pi * f2 * t_sampled)\n",
        "    line.set_data(t_sampled, signal_sampled)\n",
        "\n",
        "    # Find intersection points\n",
        "    if rate <= nyquist_rate:\n",
        "        t_intersections = np.intersect1d(t, t_sampled)\n",
        "        signal_intersections = np.sin(2 * np.pi * f1 * t_intersections) + np.sin(2 * np.pi * f2 * t_intersections)\n",
        "        points.set_data(t_intersections, signal_intersections)\n",
        "    else:\n",
        "        points.set_data([], [])\n",
        "\n",
        "    ax.set_title(f'Sampling Rate: {rate} Hz')\n",
        "\n",
        "    return line, points\n",
        "\n",
        "ani = FuncAnimation(fig, animate, init_func=init, frames=len(sampling_rates), interval=1000, blit=True)\n",
        "\n",
        "# Clear the current figure before displaying the animation\n",
        "plt.close(fig)\n",
        "\n",
        "# Display the animation\n",
        "HTML(ani.to_jshtml())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYA8YjEvv4kQ"
      },
      "source": [
        "### Missing data in time-series and resampling\n",
        "It is always a good decision to resample your data (even if it does not contain missing chunks). This will ensure the samples of the time-series are uniformly distributed through time. This is usually a must for some pre-processing steps such as filtering."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "id": "aGFY26jAunss",
        "outputId": "488307ed-0d29-49ee-a350-603c84985032"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.interpolate import interp1d\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generating example data (time and values)\n",
        "time_original_1 = np.arange(1, 9, 1/20)  # Original time array (20 Hz)\n",
        "values_original_1 = np.sin(0.5 * np.pi * time_original_1)  # Example values\n",
        "\n",
        "time_original_2 = np.arange(20, 29, 1/20)  # Original time array (20 Hz)\n",
        "values_original_2 = np.sin(2 * np.pi * time_original_2)  # Example values\n",
        "\n",
        "# Combine the two sets of example data together\n",
        "# Insert NaN to break the line between chunks\n",
        "combined_time = np.concatenate((time_original_1, [np.nan], time_original_2))\n",
        "combined_values = np.concatenate((values_original_1, [np.nan], values_original_2))\n",
        "\n",
        "# Define new time array with larger time interval (100 Hz)\n",
        "time_new = np.arange(0, 30, 1/100)  # New time array (100 Hz)\n",
        "\n",
        "# Use interp1d to interpolate and extrapolate the data\n",
        "interpolator = interp1d(combined_time, combined_values, kind='slinear', fill_value=(np.nan, np.nan), bounds_error=False)\n",
        "\n",
        "# Interpolate/extrapolate the values to the new time array\n",
        "values_new = interpolator(time_new)\n",
        "\n",
        "# Insert NaN where there are no corresponding data points in the new resampled data\n",
        "# This ensures gaps between the two chunks in the resampled plot too\n",
        "values_new[time_new < np.min(time_original_1)] = np.nan\n",
        "values_new[time_new > np.max(time_original_2)] = np.nan\n",
        "\n",
        "# Plot the original and resampled data\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Plot original signal (with NaN to break the connection between chunks)\n",
        "plt.subplot(2, 1, 1)\n",
        "plt.plot(combined_time, combined_values, label='Original Data (20 Hz)', marker='', linestyle='-', color='blue')\n",
        "plt.title('Original Data (20 Hz)')\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('Values')\n",
        "plt.grid(True)\n",
        "\n",
        "# Plot resampled signal (with NaN handling for gaps)\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.plot(time_new, values_new, label='Resampled Data (100 Hz)', marker='', linestyle='--', color='orange')\n",
        "plt.title('Resampled Data (100 Hz)')\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('Values')\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPNaA9SbENF2"
      },
      "source": [
        "### Filtering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "iAO2gWIOEPw7",
        "outputId": "02ac7f5b-a242-44ff-fd0b-211dafef9bff"
      },
      "outputs": [],
      "source": [
        "'''Trying different low pass filters'''\n",
        "\n",
        "import numpy as np\n",
        "from scipy.signal import butter,filtfilt\n",
        "from scipy.interpolate import interp1d\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generating example data (time and values)\n",
        "time_original_1 = np.arange(1, 9, 1/20)  # Original time array (20 Hz)\n",
        "values_original_1 = np.sin(0.5 * np.pi * time_original_1) + 4  # Example values\n",
        "\n",
        "time_original_2 = np.arange(20, 29, 1/20)  # Original time array (20 Hz)\n",
        "values_original_2 = np.sin(2 * np.pi * time_original_2) + 6 # Example values\n",
        "\n",
        "# Combine the two sets of example data together\n",
        "combined_time = np.concatenate((time_original_1, time_original_2))\n",
        "combined_values = np.concatenate((values_original_1, values_original_2))\n",
        "\n",
        "# Define new time array with larger time interval (100 Hz)\n",
        "time_resampled = np.arange(0, 30, 1/100)  # New time array (100 Hz)\n",
        "\n",
        "# Use interp1d to interpolate and extrapolate the data\n",
        "interpolator = interp1d(combined_time, combined_values, kind='slinear', fill_value=(0, 0), bounds_error=False)\n",
        "\n",
        "# Interpolate/extrapolate the values to the new time array\n",
        "values_resampled = interpolator(time_resampled) + 2 + np.random.normal(0, 0.5, len(time_resampled))\n",
        "\n",
        "\n",
        "def butter_lowpass_filter(data, cutoff, fs, order):\n",
        "    nyq = 0.5 * fs\n",
        "    normal_cutoff = cutoff / nyq\n",
        "    # Get the filter coefficients\n",
        "    b, a = butter(order, normal_cutoff, btype='low', analog=False)\n",
        "    y = filtfilt(b, a, data)\n",
        "    return y\n",
        "\n",
        "values_filtered = butter_lowpass_filter(values_resampled, 1.5, 100, 3)\n",
        "\n",
        "\n",
        "# Plot the original and resampled data\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(time_resampled, values_resampled, label='Before filter', marker='', linestyle='-')\n",
        "plt.plot(time_resampled, values_filtered, label='After filter', marker='', linestyle='--')\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('Values')\n",
        "plt.title('Resampled and Filtered Data')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUcWzk9d_1G5"
      },
      "source": [
        "### Spectral Analysis and Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 670
        },
        "id": "_sgzSGMy_NiS",
        "outputId": "53ce51cf-471b-40c0-8a47-838fbe444f71"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.signal import windows, spectrogram\n",
        "\n",
        "# Parameters\n",
        "fs = 1000  # Sampling frequency in Hz\n",
        "T = 1.0    # Duration in seconds\n",
        "t = np.linspace(0, T, int(T * fs), endpoint=False)  # Time vector\n",
        "\n",
        "# Create a signal with two different frequencies\n",
        "f1 = 50   # Frequency of the first sine wave (Hz)\n",
        "f2 = 120  # Frequency of the second sine wave (Hz)\n",
        "signal = np.sin(2 * np.pi * f1 * t) + 0.5 * np.sin(2 * np.pi * f2 * t)\n",
        "\n",
        "# Parameters for windowing\n",
        "window_length = 100  # Length of the window\n",
        "window_type = 'hann'  # Type of the window (e.g., 'hann', 'hamming', etc.)\n",
        "window = windows.get_window(window_type, window_length)\n",
        "n_overlap = window_length // 2  # Number of overlapping samples\n",
        "\n",
        "# Perform STFT\n",
        "frequencies, times, Sxx = spectrogram(signal, fs, window=window, nperseg=window_length, noverlap=n_overlap, scaling='spectrum')\n",
        "\n",
        "# Plot the original signal\n",
        "plt.figure(figsize=(14, 8))\n",
        "\n",
        "plt.subplot(3, 1, 1)\n",
        "plt.plot(t, signal)\n",
        "plt.title('Original Signal')\n",
        "plt.xlabel('Time [s]')\n",
        "plt.ylabel('Amplitude')\n",
        "plt.grid(True)\n",
        "\n",
        "# Plot the spectrogram\n",
        "plt.subplot(3, 1, 2)\n",
        "plt.pcolormesh(times, frequencies, 10 * np.log10(Sxx), shading='gouraud')\n",
        "plt.title('Spectrogram')\n",
        "plt.xlabel('Time [s]')\n",
        "plt.ylabel('Frequency [Hz]')\n",
        "plt.colorbar(label='Power/Frequency (dB/Hz)')\n",
        "plt.grid(True)\n",
        "\n",
        "# Illustration of windowing effect\n",
        "# Select a specific time slice for illustration\n",
        "time_slice = int(0.5 * fs) + 110  # Center of the signal (at 0.5 seconds)\n",
        "signal_slice = signal[time_slice:time_slice + window_length] * window\n",
        "\n",
        "# Perform FFT on the windowed signal slice\n",
        "N_slice = len(signal_slice)\n",
        "fft_result_slice = np.fft.fft(signal_slice)\n",
        "fft_freq_slice = np.fft.fftfreq(N_slice, 1/fs)\n",
        "\n",
        "# Only take the positive frequencies and corresponding FFT results\n",
        "positive_freqs_slice = fft_freq_slice[:N_slice//2]\n",
        "positive_fft_result_slice = fft_result_slice[:N_slice//2]\n",
        "\n",
        "# Magnitude of the FFT (normalized)\n",
        "magnitude_slice = np.abs(positive_fft_result_slice) / N_slice\n",
        "\n",
        "# Plot the FFT (magnitude spectrum) of the windowed signal slice\n",
        "plt.subplot(3, 1, 3)\n",
        "plt.stem(positive_freqs_slice, magnitude_slice, 'b', markerfmt=\" \", basefmt=\"-b\")\n",
        "plt.title('Magnitude Spectrum of a Windowed Slice')\n",
        "plt.xlabel('Frequency [Hz]')\n",
        "plt.ylabel('Magnitude')\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 670
        },
        "id": "CiAxRZKtALuC",
        "outputId": "7be1a0c9-1b92-43ea-905d-589e73a971b1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.signal import windows, spectrogram\n",
        "\n",
        "# Parameters\n",
        "fs = 1000  # Sampling frequency in Hz\n",
        "T = 1.0    # Duration in seconds\n",
        "t = np.linspace(0, T, int(T * fs), endpoint=False)  # Time vector\n",
        "\n",
        "# Create a signal with two different frequencies\n",
        "f1 = 50   # Frequency of the first sine wave (Hz)\n",
        "f2 = 120  # Frequency of the second sine wave (Hz)\n",
        "\n",
        "signal = np.sin(2 * np.pi * f1 * t) + 0.5 * np.sin(2 * np.pi * f2 * t)\n",
        "\n",
        "# Add white noise\n",
        "noise_amplitude = 0.3  # Set the amplitude of the noise\n",
        "white_noise = np.random.normal(0, noise_amplitude, size=t.shape)\n",
        "signal = signal + white_noise\n",
        "\n",
        "# Parameters for windowing\n",
        "window_length = 100  # Length of the window\n",
        "window_type = 'hann'  # Type of the window (e.g., 'hann', 'hamming', etc.)\n",
        "window = windows.get_window(window_type, window_length)\n",
        "n_overlap = window_length // 2  # Number of overlapping samples\n",
        "\n",
        "# Perform STFT\n",
        "frequencies, times, Sxx = spectrogram(signal, fs, window=window, nperseg=window_length, noverlap=n_overlap, scaling='spectrum')\n",
        "\n",
        "# Plot the original signal\n",
        "plt.figure(figsize=(14, 8))\n",
        "\n",
        "plt.subplot(3, 1, 1)\n",
        "plt.plot(t, signal)\n",
        "plt.title('Original Signal')\n",
        "plt.xlabel('Time [s]')\n",
        "plt.ylabel('Amplitude')\n",
        "plt.grid(True)\n",
        "\n",
        "# Plot the spectrogram\n",
        "plt.subplot(3, 1, 2)\n",
        "plt.pcolormesh(times, frequencies, 10 * np.log10(Sxx), shading='gouraud')\n",
        "plt.title('Spectrogram')\n",
        "plt.xlabel('Time [s]')\n",
        "plt.ylabel('Frequency [Hz]')\n",
        "plt.colorbar(label='Power/Frequency (dB/Hz)')\n",
        "plt.grid(True)\n",
        "\n",
        "# Illustration of windowing effect\n",
        "# Select a specific time slice for illustration\n",
        "time_slice = int(0.5 * fs) + 110  # Center of the signal (at 0.5 seconds)\n",
        "signal_slice = signal[time_slice:time_slice + window_length] * window\n",
        "\n",
        "# Perform FFT on the windowed signal slice\n",
        "N_slice = len(signal_slice)\n",
        "fft_result_slice = np.fft.fft(signal_slice)\n",
        "fft_freq_slice = np.fft.fftfreq(N_slice, 1/fs)\n",
        "\n",
        "# Only take the positive frequencies and corresponding FFT results\n",
        "positive_freqs_slice = fft_freq_slice[:N_slice//2]\n",
        "positive_fft_result_slice = fft_result_slice[:N_slice//2]\n",
        "\n",
        "# Magnitude of the FFT (normalized)\n",
        "magnitude_slice = np.abs(positive_fft_result_slice) / N_slice\n",
        "\n",
        "# Plot the FFT (magnitude spectrum) of the windowed signal slice\n",
        "plt.subplot(3, 1, 3)\n",
        "plt.stem(positive_freqs_slice, magnitude_slice, 'b', markerfmt=\" \", basefmt=\"-b\")\n",
        "plt.title('Magnitude Spectrum of a Windowed Slice')\n",
        "plt.xlabel('Frequency [Hz]')\n",
        "plt.ylabel('Magnitude')\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ro4LZ5HtAvMf"
      },
      "source": [
        "### Feature Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S0JfY77BAuBn",
        "outputId": "941f7d25-5760-42d1-b487-03eab607c857"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy import stats\n",
        "from scipy.signal import periodogram\n",
        "\n",
        "# Parameters\n",
        "fs = 1000  # Sampling frequency in Hz\n",
        "T = 1.0    # Duration in seconds\n",
        "t = np.linspace(0, T, int(T * fs), endpoint=False)  # Time vector\n",
        "\n",
        "# Create a signal with two different frequencies\n",
        "f1 = 50   # Frequency of the first sine wave (Hz)\n",
        "f2 = 120  # Frequency of the second sine wave (Hz)\n",
        "\n",
        "signal = np.sin(2 * np.pi * f1 * t) + 0.5 * np.sin(2 * np.pi * f2 * t)\n",
        "\n",
        "# Add white noise\n",
        "noise_amplitude = 0.3  # Set the amplitude of the noise\n",
        "white_noise = np.random.normal(0, noise_amplitude, size=t.shape)\n",
        "signal = signal + white_noise\n",
        "\n",
        "# 1. Mean of the signal\n",
        "mean_value = np.mean(signal)\n",
        "\n",
        "# 2. Standard deviation of the signal\n",
        "std_value = np.std(signal)\n",
        "\n",
        "# 3. Maximum and minimum values of the signal\n",
        "max_value = np.max(signal)\n",
        "min_value = np.min(signal)\n",
        "\n",
        "# 4. Root Mean Square (RMS)\n",
        "rms_value = np.sqrt(np.mean(signal ** 2))\n",
        "\n",
        "# 5. Skewness and kurtosis\n",
        "skewness_value = stats.skew(signal)\n",
        "kurtosis_value = stats.kurtosis(signal)\n",
        "\n",
        "# 6. Spectral Analysis (Dominant Frequency)\n",
        "frequencies, power_spectral_density = periodogram(signal, fs)\n",
        "dominant_frequency = frequencies[np.argmax(power_spectral_density)]\n",
        "\n",
        "# Display extracted features\n",
        "print(f\"Mean: {mean_value:.4f}\")\n",
        "print(f\"Standard Deviation: {std_value:.4f}\")\n",
        "print(f\"Maximum Value: {max_value:.4f}\")\n",
        "print(f\"Minimum Value: {min_value:.4f}\")\n",
        "print(f\"RMS: {rms_value:.4f}\")\n",
        "print(f\"Skewness: {skewness_value:.4f}\")\n",
        "print(f\"Kurtosis: {kurtosis_value:.4f}\")\n",
        "print(f\"Dominant Frequency: {dominant_frequency:.4f} Hz\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7EYJNMrUFAJO"
      },
      "source": [
        "### Windowing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 614
        },
        "id": "K4JVmAZWFCr2",
        "outputId": "6d9d15d7-6d43-4b28-d0d4-1bbe0b084f54"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.animation import FuncAnimation\n",
        "from IPython.display import HTML\n",
        "\n",
        "# Generate a sample signal\n",
        "np.random.seed(0)\n",
        "x = np.linspace(0, 10, 1000)\n",
        "y = np.sin(x) + 0.5 * np.random.normal(size=x.size)\n",
        "\n",
        "# Set the window size\n",
        "window_size = 200\n",
        "\n",
        "# Create the figure and axis\n",
        "fig, ax = plt.subplots()\n",
        "line, = ax.plot(x, y, label='Signal')\n",
        "window_line, = ax.plot([], [], 'r', lw=2, label='Sliding Window')\n",
        "\n",
        "# Set the axis limits\n",
        "ax.set_xlim(x.min(), x.max())\n",
        "ax.set_ylim(y.min(), y.max())\n",
        "ax.legend()\n",
        "\n",
        "# Initialize the window line\n",
        "def init():\n",
        "    window_line.set_data([], [])\n",
        "    return window_line,\n",
        "\n",
        "# Update the window line\n",
        "def update(frame):\n",
        "    start = frame\n",
        "    end = frame + window_size\n",
        "    window_line.set_data(x[start:end], y[start:end])\n",
        "    return window_line,\n",
        "\n",
        "# Create the animation\n",
        "ani = FuncAnimation(fig, update, frames=range(len(x) - window_size), init_func=init, blit=True, interval=50)\n",
        "\n",
        "plt.close(fig)\n",
        "\n",
        "# Display the animation\n",
        "HTML(ani.to_jshtml())\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
